{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0c8d25",
   "metadata": {},
   "source": [
    "# 0. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd754d",
   "metadata": {},
   "source": [
    "## 0.1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install python-binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"API_KEY\") if os.environ.get(\"API_KEY\") else \"\"\n",
    "api_secret = os.environ.get(\"API_SECRET\") if os.environ.get(\"API_SECRET\") else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877efb33",
   "metadata": {},
   "source": [
    "## 0.2. Connecting API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60663edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.binance.com\"\n",
    "api_call = \"/api/v3/ticker/price\"\n",
    "headers = {\"content-type\": \"application/json\", \"X-MBX-APIKEY\": api_key}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url + api_call, headers=headers, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    response_data = response.json()\n",
    "    print(response_data[:1])\n",
    "except requests.RequestException as e:\n",
    "    print(e)\n",
    "    response_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe786c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response_data:\n",
    "    df = pd.DataFrame.from_records(response_data)\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No data from API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff6833",
   "metadata": {},
   "source": [
    "# 1. Binance API\n",
    "\n",
    "```text\n",
    "Documentation: https://developers.binance.com/docs/binance-spot-api-docs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinanceAPI:\n",
    "    def __init__(self, api_key=None, api_secret=None):\n",
    "        self.base_url = \"https://api.binance.com\"\n",
    "        self.api_key = api_key\n",
    "        self.api_secret = api_secret\n",
    "        \n",
    "    # ข้อมูลเทียนหรือกราฟแท่งเทียนในอดีตสำหรับคู่การเทรดที่กำหนดตาม Symbol\n",
    "    def get_klines(self, symbol, interval, limit=1000, start_time=None, end_time=None):\n",
    "        endpoint = \"/api/v3/klines\"\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'limit': limit\n",
    "        }\n",
    "        \n",
    "        if start_time:\n",
    "            params['startTime'] = start_time\n",
    "        if end_time:\n",
    "            params['endTime'] = end_time\n",
    "            \n",
    "        response = requests.get(self.base_url + endpoint, params=params)\n",
    "        return response.json()\n",
    "    \n",
    "    def get_n_symbol(self, n) :\n",
    "        endpoint = \"/api/v3/ticker/price\"\n",
    "        headers = {\"content-type\": \"application/json\", \"X-MBX-APIKEY\": self.api_key}\n",
    "        response = requests.get(self.base_url + endpoint, headers=headers)\n",
    "        response = json.loads(response.text)\n",
    "        df = pd.DataFrame.from_records(response)\n",
    "        return df.loc[:n, \"symbol\"] \n",
    "    \n",
    "    def get_server_time(self, as_timestamp=False) :\n",
    "        endpoint = \"/api/v3/time\"\n",
    "        response = requests.get(self.base_url + endpoint)\n",
    "        ts = response.json()[\"serverTime\"]\n",
    "        if as_timestamp:\n",
    "            return ts\n",
    "        time = datetime.fromtimestamp(ts / 1000)\n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # สถิติการเปลี่ยนแปลงราคา 24 ชั่วโมงสำหรับคู่การเทรดที่กำหนดตาม Symbol\n",
    "    def get_24hr_ticker(self, symbol):\n",
    "        endpoint = \"/api/v3/ticker/24hr\"\n",
    "        params = {'symbol': symbol}\n",
    "        response = requests.get(self.base_url + endpoint, params=params)\n",
    "        return response.json()\n",
    "    \n",
    "    # ข้อมูล order book ปัจจุบันสำหรับคู่การเทรดที่กำหนดตาม Symbol\n",
    "    def get_orderbook(self, symbol, limit=100):\n",
    "        endpoint = \"/api/v3/depth\"\n",
    "        params = {'symbol': symbol, 'limit': limit}\n",
    "        response = requests.get(self.base_url + endpoint, params=params)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BinanceAPI(api_key, api_secret)\n",
    "api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.get_server_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.get_n_symbol(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.get_klines(\"ETHBTC\", \"1m\", limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12af73b",
   "metadata": {},
   "source": [
    "# 2. Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_historical_data(api: BinanceAPI, symbol, interval='1m', days=1, end_time=None):\n",
    "    if end_time is None:\n",
    "        end_time = int(api.get_server_time(as_timestamp=True))\n",
    "    start_time = end_time - (days * 24 * 60 * 60 * 1000)\n",
    "    \n",
    "    klines = api.get_klines(\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        'timestamp', 'open', 'high', 'low', 'close', 'volume',\n",
    "        'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "    ])\n",
    "    \n",
    "    # Convert data types\n",
    "    numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')\n",
    "    df.drop([\"ignore\"], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa310ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BinanceAPI(api_key, api_secret)\n",
    "symbol = \"ETHBTC\"\n",
    "\n",
    "train_df = collect_historical_data(api, symbol, interval=\"5m\", days=4)\n",
    "test_df = collect_historical_data(api, symbol, interval=\"5m\", days=1)\n",
    "\n",
    "test_df = test_df[test_df['timestamp'] > train_df['timestamp'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa575d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedcb98",
   "metadata": {},
   "source": [
    "<b>Columns</b>\n",
    "\n",
    "1) <b>open</b>: ราคา *แรกสุด* ที่มีการซื้อขายในช่วงเวลา t\n",
    "2) <b>high</b>: ราคา *สูงสุด* ที่มีการซื้อขายในช่วงเวลา t\n",
    "3) <b>low</b>: ราคา *ต่ำสุด* ที่มีการซื้อขายในช่วงเวลา t\n",
    "4) <b>close</b>: ราคา *สุดท้าย* ที่มีการซื้อขายในช่วงเวลา t\n",
    "\n",
    "``` \n",
    "4 Columns นี้มีการพิจารณาค่าตัวเลขเหมือนกัน เข่น 0.2389 คือ 1 ETH แลกได้ 0.2389 BTC \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "5. <b>volume</b>: จำนวนเหรียญหลักรวมที่มีการซื้อขายในช่วงเวลา t\n",
    "6. <b>quote_asset_volume</b>: จำนวนเหรียญคู่รวมที่มีการซื้อขายในช่วงเวลา t BTC รวม\n",
    "7. <b>number_of_trades</b>: จำนวนครั้งที่มีการซื้อขายในช่วงเวลา t\n",
    "8. <b>taker_buy_base_asset_volume</b>: จำนวนเหรียญหลักรวมที่มีการรีบซื้อในทันทีในช่วงเวลา t\n",
    "9. <b>taker_buy_quote_asset_volume</b>: จำนวนเหรียญคู่รวมที่มีการรีบซื้อในทันทีในช่วงเวลา t\n",
    "\n",
    "---\n",
    "\n",
    "10. <b>timestamp</b>: เวลาเริ่มต้นของการซื้อขาย\n",
    "11. <b>close_time</b>: เวลาสิ้นสุดของการซื้อขาย\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4993b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01450077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"taker_buy_base_asset_volume\"] = train_df[\"taker_buy_base_asset_volume\"].astype(float)\n",
    "train_df[\"taker_buy_quote_asset_volume\"] = train_df[\"taker_buy_quote_asset_volume\"].astype(float)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"taker_buy_base_asset_volume\"] = test_df[\"taker_buy_base_asset_volume\"].astype(float)\n",
    "test_df[\"taker_buy_quote_asset_volume\"] = test_df[\"taker_buy_quote_asset_volume\"].astype(float)\n",
    "\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f044f9",
   "metadata": {},
   "source": [
    "# 3. Create Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458984f",
   "metadata": {},
   "source": [
    "## 3.1. Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e1979",
   "metadata": {},
   "source": [
    "คำนวณค่าเฉลี่ยแบบเคลื่อนที่ทุกๆ n จุด แล้วดูแนวโน้มค่าเฉลี่ยเหล่านั้น"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dad8ce",
   "metadata": {},
   "source": [
    "- Bullish: มีแนวโน้มว่าในอนาคต ราคาสูงขึ้น -> 1 ETH มีแนวโน้มจะได้ BTC มากขึ้น\n",
    "    - ถ้าเรามี ETH อยู่ เราควรถือไว้ หรือซื้อ ETH เพิ่มเติม\n",
    "    - ถ้าเรามี BTC อยู่ เราควรขายเพื่อซื้อ ETH\n",
    "     \n",
    "- Bearish: มีแนวโน้มว่าในอนาคต ราคาลดลง -> 1 ETH มีแนวโน้มจะได้ BTC น้อยลง\n",
    "    - ถ้าเรามี ETH อยู่ เราควรขายเพื่อซื้อ BTC\n",
    "    - ถ้าเรามี BTC อยู่ เราควรถือไว้ หรือซื้อ BTC เพิ่มเติม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_moving_average(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    \n",
    "    price_col = \"close\"\n",
    "    \n",
    "    # SMA\n",
    "    for period in [20, 50, 200]:\n",
    "        df[f'SMA_{period}'] = df[price_col].rolling(period, min_periods=1).mean()\n",
    "    \n",
    "    # EMA   \n",
    "    for period in [12, 26]:\n",
    "        df[f'EMA_{period}'] = df[price_col].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # Golden Cross: 50-day SMA crosses above 200-day SMA (Long-term Bullish)\n",
    "    df['golden_cross'] = ((df['SMA_50'] > df['SMA_200']) & \n",
    "                         (df['SMA_50'].shift(1) <= df['SMA_200'].shift(1))).astype(int)\n",
    "    \n",
    "    # Death Cross: 50-day SMA crosses below 200-day SMA (Long-term Bearish)\n",
    "    df['death_cross'] = ((df['SMA_50'] < df['SMA_200']) & \n",
    "                        (df['SMA_50'].shift(1) >= df['SMA_200'].shift(1))).astype(int)\n",
    "    \n",
    "    # Bullish Cross: 20-day SMA crosses above 50-day SMA (Short-term Bullish)\n",
    "    df['bullish_cross'] = ((df['SMA_20'] > df['SMA_50']) & \n",
    "                          (df['SMA_20'].shift(1) <= df['SMA_50'].shift(1))).astype(int)\n",
    "    \n",
    "    # Bearish Cross: 20-day SMA crosses below 50-day SMA (Short-term Bearish)\n",
    "    df['bearish_cross'] = ((df['SMA_20'] < df['SMA_50']) & \n",
    "                          (df['SMA_20'].shift(1) >= df['SMA_50'].shift(1))).astype(int)\n",
    "    \n",
    "    # EMA Bullish Cross: 12-day EMA crosses above 26-day EMA (Momentum turning up)\n",
    "    df['ema_bullish_cross'] = ((df['EMA_12'] > df['EMA_26']) & \n",
    "                              (df['EMA_12'].shift(1) <= df['EMA_26'].shift(1))).astype(int)\n",
    "    \n",
    "    # EMA Bearish Cross: 12-day EMA crosses below 26-day EMA (Momentum turning down)\n",
    "    df['ema_bearish_cross'] = ((df['EMA_12'] < df['EMA_26']) & \n",
    "                              (df['EMA_12'].shift(1) >= df['EMA_26'].shift(1))).astype(int)\n",
    "    \n",
    "    # 0 = Very Bearish, 5 = Very Bullish\n",
    "    df['trend_strength'] = ((df[price_col] > df['SMA_20']).astype(int) + \n",
    "                           (df[price_col] > df['SMA_50']).astype(int) + \n",
    "                           (df[price_col] > df['SMA_200']).astype(int) +\n",
    "                           (df[price_col] > df['EMA_12']).astype(int) +\n",
    "                           (df[price_col] > df['EMA_26']).astype(int))\n",
    "    \n",
    "    # Price Distance from MAs\n",
    "    # Positive = Above MA, Negative = Below MA\n",
    "    df['price_sma20_dist'] = ((df[price_col] - df['SMA_20']) / df['SMA_20']).fillna(0)\n",
    "    df['price_sma50_dist'] = ((df[price_col] - df['SMA_50']) / df['SMA_50']).fillna(0)\n",
    "    df['price_sma200_dist'] = ((df[price_col] - df['SMA_200']) / df['SMA_200']).fillna(0)\n",
    "    \n",
    "    # Price Distance from EMAs\n",
    "    # Positive = Above MA, Negative = Below MA\n",
    "    df['price_ema12_dist'] = ((df[price_col] - df['EMA_12']) / df['EMA_12']).fillna(0)\n",
    "    df['price_ema26_dist'] = ((df[price_col] - df['EMA_26']) / df['EMA_26']).fillna(0)\n",
    "    \n",
    "    # Bullish Alignment: SMA_20 > SMA_50 > SMA_200\n",
    "    df['bullish_alignment'] = ((df['SMA_20'] > df['SMA_50']) & \n",
    "                              (df['SMA_50'] > df['SMA_200'])).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb21f21",
   "metadata": {},
   "source": [
    "## 3.2. Relative Strength Index (RSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790ef08",
   "metadata": {},
   "source": [
    "https://www.investopedia.com/terms/r/rsi.asp\n",
    "- บ่งบอกความแรงของราคา (Momentum) ในช่วงเวลาที่กำหนด ซึ่งมักนิยมใช้ 14 วัน\n",
    "- มีค่าอยู่ในช่วงระหว่าง 0 ถึง 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc37c0d",
   "metadata": {},
   "source": [
    "พิจารณา ETHBTC\n",
    "- RSI สูง (>70) = คนซื้อ ETH ด้วย BTC เยอะมาก \n",
    "    - ETH อาจแพงเกินไปที่จะซื้อตอนนี้ \n",
    "    - ตอนนี้เราควรขาย ETH และซื้อ BTC\n",
    "- RSI ต่ำ (<30) = คนขาย ETH เพื่อซื้อ BTC เยอะมาก \n",
    "    - ETH อาจถูกเกินไปที่จะขายตอนนี้ \n",
    "    - ตอนนี้เราควรซื้อ ETH และขาย BTC\n",
    "- RSI กลางๆ (~50) = การซื้อขาย ETH/BTC ปกติดี  \n",
    "    - ตอนนี้ควรรอดูสถานการณ์ก่อน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rsi(df:pd.DataFrame, period=14):\n",
    "    df = df.copy()\n",
    "    \n",
    "    price_col='close'\n",
    "    \n",
    "    delta = df[price_col].diff()\n",
    "    \n",
    "    gains = delta.where(delta > 0, 0)\n",
    "    losses = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    avg_gains = gains.ewm(alpha=1/period, adjust=False).mean()\n",
    "    avg_losses = losses.ewm(alpha=1/period, adjust=False).mean()\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rs = avg_gains / avg_losses\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['rsi'] = rsi\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b48b7d",
   "metadata": {},
   "source": [
    "## 3.3. MACD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93bffa",
   "metadata": {},
   "source": [
    "- ใช้ดูแนวโน้ม (trend) และโมเมนตัม (momentum) ของราคา \n",
    "- ดูจากความแตกต่างของค่าเฉลี่ยเคลื่อนที่แบบ EMA สองเส้น (fast: EMA12 ลบกับ slow: EMA26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5c724",
   "metadata": {},
   "source": [
    "พิจารณา ETHBTC\n",
    "- MACD > 0 แสดงว่าราคากำลังขึ้น หรือก็คือมีแนวโน้มว่าในอนาคต BTC จะมีราคาสูงขึ้น\n",
    "    - ถ้าเราถือ ETH เราควรถือไว้ รอขายในอนาคต\n",
    "    - ถ้าเราถือ BTC เราควรขายเพื่อซื้อ ETH\n",
    "- MACD < 0 แสดงว่าราคากำลังลง หรือก็คือมีแนวโน้มว่าในอนาคต BTC จะมีราคาต่ำขึ้น\n",
    "    - ถ้าเราถือ ETH เราควรขายเพื่อซื้อ BTC\n",
    "    - ถ้าเราถือ BTC เราควรถือไว้ รอขายในอนาคต\n",
    "\n",
    "** ยิ่งค่าห่างจาก 0 ยิ่งมีแนวโน้มที่จะไปทางนั้นๆ สูง\n",
    "\n",
    "- จังหวะที่ควรซื้อ หรือขาย คือจังหวะที่เส้นของ MACD ตัดกับเส้น MACD_Signal\n",
    "    - MACD ตัดแล้วขึ้นสูงกว่า MACD_Signal: เป็นช่วงราคากำลังขึ้น\n",
    "    - MACD ตัดแล้วต่ำกว่า MACD_Signal: เป็นช่วงราคากำลังลง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5832f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_macd(df, fast=12, slow=26, signal=9):\n",
    "    # Calculate EMA fast and slow\n",
    "    df['ema_fast'] = df['close'].ewm(span=fast, adjust=False).mean()\n",
    "    df['ema_slow'] = df['close'].ewm(span=slow, adjust=False).mean()\n",
    "    \n",
    "    # MACD line\n",
    "    df['macd'] = df['ema_fast'] - df['ema_slow']\n",
    "    \n",
    "    # Signal line\n",
    "    df['macd_signal'] = df['macd'].ewm(span=signal, adjust=False).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce99a58",
   "metadata": {},
   "source": [
    "## 3.4. Bollinger Bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c45401",
   "metadata": {},
   "source": [
    "- ประกอบด้วย 3 เส้นหลัก:\n",
    "\n",
    "1. เส้นกลาง (Middle Band) : SMA ของ close หมายถึงแนวโน้มราคากลาง ๆ ในช่วงเวลาที่กำหนด\n",
    "\n",
    "2. เส้นบน (Upper Band): SMA + 2SD บ่งบอกขอบเขตราคาที่ \"สูงกว่าปกติ\" หรือเป็นระดับแนวต้าน\n",
    "\n",
    "3. เส้นล่าง (Lower Band): SMA - 2SD บ่งบอกขอบเขตราคาที่ \"ต่ำกว่าปกติ\" หรือเป็นระดับแนวรับ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516ee52",
   "metadata": {},
   "source": [
    "พิจารณา ETHBTC\n",
    "- มี 4 Case ที่เกิดขึ้นได้\n",
    "    1. Close Price ชิด Upper Band: ราคาสูงเกินไปแล้ว อาจมีโอกาสราคาตกลงในเร็ว ๆ นี้\n",
    "        - ถ้าเราถือ ETH ควรขายเพื่อซื้อ BTC\n",
    "        - ถ้าเราถือ BTC ควรถือไว้ รอขายในอนาคต\n",
    "    2. Close Price ชิด Lower Band: ราคาต่ำเกินไปแล้ว อาจมีโอกาสราคาขึ้นในเร็ว ๆ นี้\n",
    "        - ถ้าเราถือ ETH ควรถือไว้ รอขายในอนาคต\n",
    "        - ถ้าเราถือ BTC ควรขายเพื่อซื้อ ETH\n",
    "    3. Upper Band กับ Lower Band เข้ามาชิดกัน: ความผันผวนต่ำ เตรียมเคลื่อนไหว\n",
    "        - ถ้า Close Price สูงกว่า Upper Band อาจเป็นสัญญาณซื้อ ETH ขาย BTC\n",
    "        - ถ้า Close Price ต่ำกว่า Lower Band อาจเป็นสัญญาณขาย ETH ซื้อ BTC\n",
    "    4. Upper Band กับ Lower Band ห่างออกจากกัน: ความผันผวนสูง\n",
    "        - ถ้า Close Price สูงขึ้น และอยู่ใกล้ Upper Band ควรถือ ETH ต่อเนื่อง หรือซื้อเพิ่ม\n",
    "        - ถ้า Close Price ลดลง และอยู่ใกล้ Lower Band → ควรขาย ETH ซื้อ BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566235de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bollinger(df, period=20, std_dev=2):\n",
    "    df['bb_middle'] = df['close'].rolling(window=period).mean()\n",
    "    df['bb_std'] = df['close'].rolling(window=period).std()\n",
    "    \n",
    "    df['bb_upper'] = df['bb_middle'] + std_dev * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_middle'] - std_dev * df['bb_std']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_indicators(df):\n",
    "    df = df.copy() \n",
    "    df = add_moving_average(df)\n",
    "    df = add_rsi(df)\n",
    "    df = add_macd(df)\n",
    "    df = add_bollinger(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef472f",
   "metadata": {},
   "source": [
    "# 4. Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c1c3d",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df = add_all_indicators(train_df)\n",
    "ind_train_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "ind_test_df = add_all_indicators(test_df)\n",
    "ind_test_df.set_index(\"timestamp\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea907810",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bb_middle\\n\", ind_train_df[\"bb_middle\"])\n",
    "print(\"bb_std\\n\", ind_train_df[\"bb_std\"])\n",
    "print(\"bb_upper\\n\", ind_train_df[\"bb_upper\"])\n",
    "print(\"bb_lower\\n\", ind_train_df[\"bb_lower\"])\n",
    "print(\"rsi\\n\", ind_train_df[\"rsi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df['price_direction'] = np.where(\n",
    "    ind_train_df['close'].shift(-1) < ind_train_df['close'], -1, 1\n",
    ")\n",
    "ind_train_df['price_direction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8902798",
   "metadata": {},
   "source": [
    "- -1 คือ ราคาปิดในอนาคต ***น้อยกว่า*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรขาย\n",
    "- 1 คือ ราคาปิดในอนาคต ***มากกว่าหรือเท่ากับ*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรซื้อ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d660850",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bb_middle\\n\", ind_test_df[\"bb_middle\"])\n",
    "print(\"bb_std\\n\", ind_test_df[\"bb_std\"])\n",
    "print(\"bb_upper\\n\", ind_test_df[\"bb_upper\"])\n",
    "print(\"bb_lower\\n\", ind_test_df[\"bb_lower\"])\n",
    "print(\"rsi\\n\", ind_test_df[\"rsi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955944a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df['price_direction'] = np.where(\n",
    "    ind_test_df['close'].shift(-1) < ind_test_df['close'], -1, 1\n",
    ")\n",
    "ind_test_df['price_direction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba89c5",
   "metadata": {},
   "source": [
    "- -1 คือ ราคาปิดในอนาคต ***น้อยกว่า*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรขาย\n",
    "- 1 คือ ราคาปิดในอนาคต ***มากกว่าหรือเท่ากับ*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรซื้อ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48403ec",
   "metadata": {},
   "source": [
    "## Preparing to Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df['price_direction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "optimal_jobs = max(1, n_cores // 3)\n",
    "\n",
    "random_state = 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02876a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "features = [\n",
    "    'SMA_20', 'SMA_50', 'SMA_200',\n",
    "    'EMA_12', 'EMA_26',\n",
    "    'rsi', \n",
    "    'macd', 'macd_signal',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower',\n",
    "    'trend_strength', 'price_sma20_dist', 'price_sma50_dist',\n",
    "    'price_sma200_dist',\n",
    "    'price_ema12_dist', 'price_ema26_dist', 'bullish_alignment'\n",
    "]\n",
    "\n",
    "target = 'price_direction'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be587dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, features, target, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # แปลงค่า prediction\n",
    "    if set(np.unique(y_test)).issubset({-1, 1}) and set(np.unique(y_pred)).issubset({0, 1}):\n",
    "        y_pred = np.where(y_pred == 0, -1, 1)\n",
    "        \n",
    "    if set(np.unique(y_test)).issubset({0, 1}):\n",
    "        y_pred = np.where(y_pred == 0, -1, 1)\n",
    "        y_test = np.where(y_test == 0, -1, 1)\n",
    "    \n",
    "    # ตรวจสอบ prediction diversity\n",
    "    unique_preds = np.unique(y_pred)\n",
    "    \n",
    "    # ตรวจสอบ class distribution ใน predictions\n",
    "    pred_counts = pd.Series(y_pred).value_counts(normalize=True)\n",
    "    min_pred_ratio = pred_counts.min()\n",
    "        \n",
    "    # คำนวณ metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    precision_all = precision_score(y_test, y_pred, average=None, labels=[-1, 1], zero_division=0)\n",
    "    recall_all = recall_score(y_test, y_pred, average=None, labels=[-1, 1], zero_division=0)\n",
    "    f1_all = f1_score(y_test, y_pred, average=None, labels=[-1, 1], zero_division=0)\n",
    "    \n",
    "    precision_sell = precision_all[0] if len(precision_all) > 0 else 0\n",
    "    precision_buy = precision_all[1] if len(precision_all) > 1 else 0\n",
    "    recall_sell = recall_all[0] if len(recall_all) > 0 else 0\n",
    "    recall_buy = recall_all[1] if len(recall_all) > 1 else 0\n",
    "    f1_sell = f1_all[0] if len(f1_all) > 0 else 0\n",
    "    f1_buy = f1_all[1] if len(f1_all) > 1 else 0\n",
    "    \n",
    "    # ROC AUC\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_score = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_score = model.decision_function(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, y_score)\n",
    "    except:\n",
    "        roc_auc = 0.5\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"features\": features,\n",
    "        \"target\": target,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_sell\": precision_sell,\n",
    "        \"precision_buy\": precision_buy,  \n",
    "        \"recall_sell\": recall_sell,\n",
    "        \"recall_buy\": recall_buy,\n",
    "        \"f1_sell\": f1_sell,\n",
    "        \"f1_buy\": f1_buy,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"prediction_diversity\": len(unique_preds),\n",
    "        \"min_pred_ratio\": min_pred_ratio\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_results(lr_evaluation, rf_evaluation, xgb_evaluation, knn_evaluation, svm_evaluation): \n",
    "    data = {\n",
    "        \"model\": [\"LR\", \"RF\", \"XGB\", \"KNN\", \"SVM\"],\n",
    "        \"accuracy\": [lr_evaluation[\"accuracy\"], rf_evaluation[\"accuracy\"], \n",
    "                    xgb_evaluation[\"accuracy\"], knn_evaluation[\"accuracy\"], \n",
    "                    svm_evaluation[\"accuracy\"]],\n",
    "        \"precision_sell\": [lr_evaluation[\"precision_sell\"], rf_evaluation[\"precision_sell\"],\n",
    "                            xgb_evaluation[\"precision_sell\"], knn_evaluation[\"precision_sell\"], \n",
    "                            svm_evaluation[\"precision_sell\"]],\n",
    "        \"precision_buy\": [lr_evaluation[\"precision_buy\"], rf_evaluation[\"precision_buy\"],\n",
    "                            xgb_evaluation[\"precision_buy\"], knn_evaluation[\"precision_buy\"],\n",
    "                            svm_evaluation[\"precision_buy\"]],\n",
    "        \"recall_sell\": [lr_evaluation[\"recall_sell\"], rf_evaluation[\"recall_sell\"],\n",
    "                            xgb_evaluation[\"recall_sell\"], knn_evaluation[\"recall_sell\"],\n",
    "                            svm_evaluation[\"recall_sell\"]],\n",
    "        \"recall_buy\": [lr_evaluation[\"recall_buy\"], rf_evaluation[\"recall_buy\"],\n",
    "                            xgb_evaluation[\"recall_buy\"], knn_evaluation[\"recall_buy\"],\n",
    "                            svm_evaluation[\"recall_buy\"]],\n",
    "        \"f1_sell\": [lr_evaluation[\"f1_sell\"], rf_evaluation[\"f1_sell\"],\n",
    "                        xgb_evaluation[\"f1_sell\"], knn_evaluation[\"f1_sell\"],\n",
    "                        svm_evaluation[\"f1_sell\"]],\n",
    "        \"f1_buy\": [lr_evaluation[\"f1_buy\"], rf_evaluation[\"f1_buy\"],\n",
    "                        xgb_evaluation[\"f1_buy\"], knn_evaluation[\"f1_buy\"],\n",
    "                        svm_evaluation[\"f1_buy\"]],\n",
    "        \"roc_auc\": [lr_evaluation[\"roc_auc\"], rf_evaluation[\"roc_auc\"],\n",
    "                    xgb_evaluation[\"roc_auc\"], knn_evaluation[\"roc_auc\"],\n",
    "                    svm_evaluation[\"roc_auc\"]],\n",
    "        # เพิ่ม columns ที่ขาดหายไป\n",
    "        \"prediction_diversity\": [lr_evaluation[\"prediction_diversity\"], rf_evaluation[\"prediction_diversity\"],\n",
    "                                xgb_evaluation[\"prediction_diversity\"], knn_evaluation[\"prediction_diversity\"],\n",
    "                                svm_evaluation[\"prediction_diversity\"]],\n",
    "        \"min_pred_ratio\": [lr_evaluation[\"min_pred_ratio\"], rf_evaluation[\"min_pred_ratio\"],\n",
    "                          xgb_evaluation[\"min_pred_ratio\"], knn_evaluation[\"min_pred_ratio\"],\n",
    "                          svm_evaluation[\"min_pred_ratio\"]]\n",
    "    }\n",
    "    \n",
    "    results_df = pd.DataFrame(data)\n",
    "    results_df.set_index(\"model\", inplace=True)\n",
    "    return results_df, {\"LR\": lr_evaluation[\"features\"], \"RF\": rf_evaluation[\"features\"],\n",
    "                        \"XGB\": xgb_evaluation[\"features\"], \"KNN\": knn_evaluation[\"features\"],\n",
    "                        \"SVM\": svm_evaluation[\"features\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(X, y, test_size=0.2):    \n",
    "    min_test_size = max(1, int(len(X) * 0.1))\n",
    "    actual_test_size = min(test_size, min_test_size / len(X))\n",
    "    \n",
    "    return train_test_split(X, y, test_size=actual_test_size, shuffle=False, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b9333",
   "metadata": {},
   "source": [
    "## Model Training V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4224ffd",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01679741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train model\n",
    "    model = LogisticRegression(random_state=random_state, class_weight='balanced', max_iter=1000)\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # 3. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376c2f7",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train model\n",
    "    model = RandomForestClassifier(random_state=random_state, class_weight='balanced')\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # 3. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec22b04",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab919f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target].map({-1: 0, 1: 1})\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train model บนข้อมูลทั้งหมด\n",
    "    scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "    model = XGBClassifier(random_state=random_state, scale_pos_weight=scale_pos_weight)\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # 3. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea426b",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1093c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train model บนข้อมูลทั้งหมด\n",
    "    model = KNeighborsClassifier(weights='distance')\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # 3. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c918593",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad38b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train model บนข้อมูลทั้งหมด\n",
    "    model = SVC(random_state=random_state, class_weight='balanced')\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # 3. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9a08f",
   "metadata": {},
   "source": [
    "## Model Summary V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_v1_result, lr_scaler_v1_result = lr_v1(ind_train_df)\n",
    "rf_model_v1_result, rf_scaler_v1_result = rf_v1(ind_train_df)\n",
    "xgb_model_v1_result, xgb_scaler_v1_result = xgb_v1(ind_train_df)\n",
    "knn_model_v1_result, knn_scaler_v1_result = knn_v1(ind_train_df)\n",
    "svm_model_v1_result, svm_scaler_v1_result = svm_v1(ind_train_df)\n",
    "\n",
    "X_test = ind_test_df[features]\n",
    "y_test = ind_test_df[target]\n",
    "\n",
    "X_test_lr = lr_scaler_v1_result.transform(X_test)\n",
    "X_test_rf = rf_scaler_v1_result.transform(X_test)\n",
    "X_test_xgb = xgb_scaler_v1_result.transform(X_test)\n",
    "X_test_knn = knn_scaler_v1_result.transform(X_test)\n",
    "X_test_svm = svm_scaler_v1_result.transform(X_test)\n",
    "\n",
    "y_test_xgb = y_test.map({-1: 0, 1: 1})\n",
    "\n",
    "lr_evaluation = model_evaluation(lr_model_v1_result, features, target, X_test_lr, y_test)\n",
    "rf_evaluation = model_evaluation(rf_model_v1_result, features, target, X_test_rf, y_test)\n",
    "xgb_evaluation = model_evaluation(xgb_model_v1_result, features, target, X_test_xgb, y_test_xgb)\n",
    "knn_evaluation = model_evaluation(knn_model_v1_result, features, target, X_test_knn, y_test)\n",
    "svm_evaluation = model_evaluation(svm_model_v1_result, features, target, X_test_svm, y_test)\n",
    "\n",
    "results_df_v1, result_features_v1 = show_model_results(lr_evaluation, rf_evaluation, xgb_evaluation, \n",
    "                                                       knn_evaluation, svm_evaluation)\n",
    "\n",
    "results_df_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29465d",
   "metadata": {},
   "source": [
    "## Model Training V2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab675a8",
   "metadata": {},
   "source": [
    "- เพิ่ม Hyperparameter Tuning \n",
    "- เพิ่ม SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29188cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "features = [\n",
    "    'SMA_20', 'SMA_50', 'SMA_200',\n",
    "    'EMA_12', 'EMA_26',\n",
    "    'rsi', \n",
    "    'macd', 'macd_signal',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower',\n",
    "    'trend_strength', 'price_sma20_dist', 'price_sma50_dist',\n",
    "    'price_sma200_dist',\n",
    "    'price_ema12_dist', 'price_ema26_dist', 'bullish_alignment'\n",
    "]\n",
    "\n",
    "target = 'price_direction' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c43020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_smote(X_train, y_train):\n",
    "    try:\n",
    "        counter = Counter(y_train)\n",
    "        min_class_count = min(counter.values())\n",
    "        max_class_count = max(counter.values())\n",
    "        imbalance_ratio = max_class_count / min_class_count\n",
    "        \n",
    "        if imbalance_ratio > 2.0 and min_class_count >= 10:  \n",
    "            k_neighbors = min(3, min_class_count - 1)\n",
    "            sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "            X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "            return X_resampled, y_resampled\n",
    "        else:\n",
    "            return X_train, y_train\n",
    "    except Exception as e:\n",
    "        return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f434b64",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE)\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000, random_state=random_state), \n",
    "        param_grid, \n",
    "        n_jobs=optimal_jobs,\n",
    "        cv=3,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_res, y_res)\n",
    "\n",
    "    # 4. Train final model บนข้อมูลทั้งหมด\n",
    "    model = LogisticRegression(max_iter=1000, **grid.best_params_, random_state=random_state)\n",
    "    model.fit(X_res, y_res)\n",
    "\n",
    "    # 5. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be01378",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE)\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [4, 8, 16, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=random_state), \n",
    "        param_grid,\n",
    "        n_jobs=optimal_jobs,\n",
    "        cv=3,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_res, y_res)\n",
    "\n",
    "    # 4. Train final model บนข้อมูลทั้งหมด\n",
    "    model = RandomForestClassifier(random_state=random_state, **grid.best_params_)\n",
    "    model.fit(X_res, y_res)\n",
    "\n",
    "    # 5. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92efc840",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target].map({-1: 0, 1: 1})\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE)\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        XGBClassifier(eval_metric='logloss', random_state=random_state),\n",
    "        param_grid, \n",
    "        n_jobs=optimal_jobs,\n",
    "        cv=3,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_res, y_res)\n",
    "\n",
    "    # 4. Train final model บนข้อมูลทั้งหมด\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        random_state=random_state, **grid.best_params_)\n",
    "    model.fit(X_res, y_res)\n",
    "\n",
    "    # 5. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10684d33",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefe2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE)\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "    }\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                        param_grid, \n",
    "                        n_jobs=optimal_jobs,\n",
    "                        cv=3,\n",
    "                        scoring='accuracy')\n",
    "    grid.fit(X_res, y_res)\n",
    "\n",
    "    # 4. Train final model บนข้อมูลทั้งหมด\n",
    "    model = KNeighborsClassifier(**grid.best_params_)\n",
    "    model.fit(X_res, y_res)\n",
    "\n",
    "    # 5. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb6f1b",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE)\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'C': [1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "    grid = GridSearchCV(SVC(), \n",
    "                        param_grid, \n",
    "                        n_jobs=optimal_jobs,\n",
    "                        cv=3,\n",
    "                        scoring='accuracy')\n",
    "    grid.fit(X_res, y_res)\n",
    "\n",
    "    # 4. Train final model บนข้อมูลทั้งหมด\n",
    "    model = SVC(**grid.best_params_, random_state=random_state)\n",
    "    model.fit(X_res, y_res)\n",
    "\n",
    "    # 5. Return model และ scaler\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b558c",
   "metadata": {},
   "source": [
    "## Model Result V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05550555",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_v2_result, lr_scaler_v2_result = lr_v2(ind_train_df)\n",
    "rf_model_v2_result, rf_scaler_v2_result = rf_v2(ind_train_df)\n",
    "xgb_model_v2_result, xgb_scaler_v2_result = xgb_v2(ind_train_df)\n",
    "knn_model_v2_result, knn_scaler_v2_result = knn_v2(ind_train_df)\n",
    "svm_model_v2_result, svm_scaler_v2_result = svm_v2(ind_train_df)\n",
    "\n",
    "X_test = ind_test_df[features]\n",
    "y_test = ind_test_df[target]\n",
    "\n",
    "X_test_lr = lr_scaler_v2_result.transform(X_test)\n",
    "X_test_rf = rf_scaler_v2_result.transform(X_test)\n",
    "X_test_xgb = xgb_scaler_v2_result.transform(X_test)\n",
    "X_test_knn = knn_scaler_v2_result.transform(X_test)\n",
    "X_test_svm = svm_scaler_v2_result.transform(X_test)\n",
    "\n",
    "y_test_xgb = y_test.map({-1: 0, 1: 1})\n",
    "\n",
    "lr_evaluation_v2 = model_evaluation(lr_model_v2_result, features, target, X_test_lr, y_test)\n",
    "rf_evaluation_v2 = model_evaluation(rf_model_v2_result, features, target, X_test_rf, y_test)\n",
    "xgb_evaluation_v2 = model_evaluation(xgb_model_v2_result, features, target, X_test_xgb, y_test_xgb)\n",
    "knn_evaluation_v2 = model_evaluation(knn_model_v2_result, features, target, X_test_knn, y_test)\n",
    "svm_evaluation_v2 = model_evaluation(svm_model_v2_result, features, target, X_test_svm, y_test)\n",
    "\n",
    "results_df_v2, result_features_v2 = show_model_results(lr_evaluation_v2, \n",
    "                                                       rf_evaluation_v2, xgb_evaluation_v2, \n",
    "                                                       knn_evaluation_v2, svm_evaluation_v2)\n",
    "\n",
    "results_df_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92794e",
   "metadata": {},
   "source": [
    "## Model Training V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70419385",
   "metadata": {},
   "source": [
    "- เพิ่มการทำ Dynamic Features Selection\n",
    "- เพิ่ม Class Weight Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'SMA_20', 'SMA_50', 'SMA_200',\n",
    "    'EMA_12', 'EMA_26',\n",
    "    'rsi', \n",
    "    'macd', 'macd_signal',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower',\n",
    "    'trend_strength', 'price_sma20_dist', 'price_sma50_dist',\n",
    "    'price_sma200_dist',\n",
    "    'price_ema12_dist', 'price_ema26_dist', 'bullish_alignment'\n",
    "]\n",
    "\n",
    "target = 'price_direction' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ffdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_by_coef(model, top_n=10):\n",
    "    coef_abs = np.abs(model.coef_[0])\n",
    "    feature_importance = pd.Series(coef_abs, index=features)\n",
    "    top_feature_importance = feature_importance.sort_values(ascending=False).head(top_n)\n",
    "    return top_feature_importance.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_by_importance(model, top_n=10):\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance = pd.Series(importances, index=features)\n",
    "    top_feature_importance = feature_importance.sort_values(ascending=False).head(top_n)\n",
    "    return top_feature_importance.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf17f52",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE) ก่อน feature selection\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Fit model ด้วย features ทั้งหมดเพื่อหา top features\n",
    "    model_all = LogisticRegression(max_iter=1000, random_state=random_state, class_weight='balanced')\n",
    "    model_all.fit(X_res, y_res)\n",
    "\n",
    "    # 4. เลือก top N feature (เพิ่มจำนวน features)\n",
    "    top_features = get_top_features_by_coef(model_all, top_n=min(15, len(features)))\n",
    "\n",
    "    # 5. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    scaler_top = StandardScaler()  # ใช้ scaler ใหม่\n",
    "    X_top_scaled = scaler_top.fit_transform(X_top)\n",
    "\n",
    "    # 6. Balance Data (SMOTE) สำหรับ Top Features\n",
    "    X_top_res, y_top_res = custom_smote(X_top_scaled, y)\n",
    "\n",
    "    # 7. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'class_weight': ['balanced', {-1: 2, 1: 1}, {-1: 1, 1: 2}]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        grid = GridSearchCV(\n",
    "            LogisticRegression(max_iter=1000, random_state=random_state), \n",
    "            param_grid, \n",
    "            n_jobs=optimal_jobs,\n",
    "            cv=3,\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        grid.fit(X_top_res, y_top_res)\n",
    "        best_params = grid.best_params_\n",
    "    except:\n",
    "        best_params = {'C': 1, 'solver': 'lbfgs', 'class_weight': 'balanced'}\n",
    "\n",
    "    # 8. Train final model\n",
    "    model = LogisticRegression(max_iter=1000, **best_params, random_state=random_state)\n",
    "    model.fit(X_top_res, y_top_res)\n",
    "\n",
    "    # 9. Return model, scaler, และ top_features\n",
    "    return model, scaler_top, top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb184b2",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE) ก่อน feature selection\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Fit model ด้วย features ทั้งหมดเพื่อหา top features\n",
    "    model_all = RandomForestClassifier(random_state=random_state, class_weight='balanced')\n",
    "    model_all.fit(X_res, y_res)\n",
    "\n",
    "    # 4. เลือก top N feature (เพิ่มจำนวน features)\n",
    "    top_features = get_top_features_by_importance(model_all, top_n=min(15, len(features)))\n",
    "\n",
    "    # 5. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    scaler_top = StandardScaler()  # ใช้ scaler ใหม่\n",
    "    X_top_scaled = scaler_top.fit_transform(X_top)\n",
    "\n",
    "    # 6. Balance Data (SMOTE) สำหรับ Top Features\n",
    "    X_top_res, y_top_res = custom_smote(X_top_scaled, y)\n",
    "\n",
    "    # 7. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [4, 8, 16, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight': ['balanced', {-1: 2, 1: 1}, {-1: 1, 1: 2}]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        grid = GridSearchCV(\n",
    "            RandomForestClassifier(random_state=random_state),\n",
    "            param_grid, \n",
    "            n_jobs=optimal_jobs,\n",
    "            cv=3,\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        grid.fit(X_top_res, y_top_res)\n",
    "        best_params = grid.best_params_\n",
    "    except:\n",
    "        best_params = {'n_estimators': 100, 'max_depth': 8, 'min_samples_split': 5, 'class_weight': 'balanced'}\n",
    "\n",
    "    # 8. Train final model\n",
    "    model = RandomForestClassifier(random_state=random_state, **best_params)\n",
    "    model.fit(X_top_res, y_top_res)\n",
    "    \n",
    "    # 9. Return model, scaler, และ top_features\n",
    "    return model, scaler_top, top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92c363",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target].map({-1: 0, 1: 1})\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE) ก่อน feature selection\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Fit model ด้วย features ทั้งหมดเพื่อหา top features\n",
    "    model_all = XGBClassifier(eval_metric='logloss', random_state=random_state)\n",
    "    model_all.fit(X_res, y_res)\n",
    "\n",
    "    # 4. เลือก top N feature (เพิ่มจำนวน features)\n",
    "    top_n = min(15, len(features))\n",
    "    top_features = get_top_features_by_importance(model_all, top_n=top_n)\n",
    "\n",
    "    # 5. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    scaler_top = StandardScaler()  # ใช้ scaler ใหม่\n",
    "    X_top_scaled = scaler_top.fit_transform(X_top)\n",
    "\n",
    "    # 6. Balance Data (SMOTE) สำหรับ Top Features\n",
    "    X_top_res, y_top_res = custom_smote(X_top_scaled, y)\n",
    "\n",
    "    # 7. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        grid = GridSearchCV(\n",
    "            XGBClassifier(eval_metric='logloss', random_state=random_state),\n",
    "            param_grid, \n",
    "            n_jobs=optimal_jobs,\n",
    "            cv=3,\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        grid.fit(X_top_res, y_top_res)\n",
    "        best_params = grid.best_params_\n",
    "    except:\n",
    "        best_params = {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8}\n",
    "\n",
    "    # 8. Train final model\n",
    "    model = XGBClassifier(eval_metric='logloss', random_state=random_state, **best_params)\n",
    "    model.fit(X_top_res, y_top_res)\n",
    "\n",
    "    # 9. Return model, scaler, และ top_features\n",
    "    return model, scaler_top, top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf029730",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def knn_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE) ก่อน feature selection\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Feature Selection ด้วย SelectKBest\n",
    "    top_n = min(15, len(features))\n",
    "    selector = SelectKBest(score_func=f_classif, k=top_n)\n",
    "    X_selected = selector.fit_transform(X_res, y_res)\n",
    "    mask = selector.get_support()\n",
    "    top_features = [feature for feature, selected in zip(features, mask) if selected]\n",
    "\n",
    "    # 4. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    scaler_top = StandardScaler()  # ใช้ scaler ใหม่\n",
    "    X_top_scaled = scaler_top.fit_transform(X_top)\n",
    "\n",
    "    # 5. Balance Data (SMOTE) สำหรับ Top Features\n",
    "    X_top_res, y_top_res = custom_smote(X_top_scaled, y)\n",
    "\n",
    "    # 6. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan'],        \n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                            param_grid,\n",
    "                            n_jobs=optimal_jobs,\n",
    "                            cv=3,\n",
    "                            scoring='accuracy')\n",
    "        grid.fit(X_top_res, y_top_res)\n",
    "        best_params = grid.best_params_\n",
    "    except:\n",
    "        best_params = {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
    "\n",
    "    # 7. Train final model\n",
    "    model = KNeighborsClassifier(**best_params)\n",
    "    model.fit(X_top_res, y_top_res)\n",
    "\n",
    "    # 8. Return model, scaler, และ top_features\n",
    "    return model, scaler_top, top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a7584",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Balance Data (SMOTE) ก่อน feature selection\n",
    "    X_res, y_res = custom_smote(X_scaled, y)\n",
    "\n",
    "    # 3. Fit model ด้วย features ทั้งหมดเพื่อหา top features\n",
    "    model_all = SVC(kernel='linear', class_weight='balanced', random_state=random_state)\n",
    "    model_all.fit(X_res, y_res)\n",
    "\n",
    "    # 4. เลือก top N feature (เพิ่มจำนวน features)\n",
    "    top_features = get_top_features_by_coef(model_all, top_n=min(15, len(features)))\n",
    "\n",
    "    # 5. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    scaler_top = StandardScaler()  # ใช้ scaler ใหม่\n",
    "    X_top_scaled = scaler_top.fit_transform(X_top)\n",
    "\n",
    "    # 6. Balance Data (SMOTE) สำหรับ Top Features\n",
    "    X_top_res, y_top_res = custom_smote(X_top_scaled, y)\n",
    "\n",
    "    # 7. Hyperparameter Tuning with Cross-Validation\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': ['balanced', {-1: 2, 1: 1}, {-1: 1, 1: 2}]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        grid = GridSearchCV(SVC(random_state=random_state), \n",
    "                            param_grid, \n",
    "                            n_jobs=optimal_jobs,\n",
    "                            cv=3,\n",
    "                            scoring='accuracy')\n",
    "        grid.fit(X_top_res, y_top_res)\n",
    "        best_params = grid.best_params_\n",
    "    except:\n",
    "        best_params = {'C': 1, 'kernel': 'rbf', 'gamma': 'scale', 'class_weight': 'balanced'}\n",
    "\n",
    "    # 8. Train final model\n",
    "    model = SVC(**best_params, random_state=random_state)\n",
    "    model.fit(X_top_res, y_top_res)\n",
    "\n",
    "    # 9. Return model, scaler, และ top_features\n",
    "    return model, scaler_top, top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d563d",
   "metadata": {},
   "source": [
    "## Model Result V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044944",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_v3_result, lr_scaler_v3_result, lr_features_v3 = lr_v3(ind_train_df)\n",
    "rf_model_v3_result, rf_scaler_v3_result, rf_features_v3 = rf_v3(ind_train_df)\n",
    "xgb_model_v3_result, xgb_scaler_v3_result, xgb_features_v3 = xgb_v3(ind_train_df)\n",
    "knn_model_v3_result, knn_scaler_v3_result, knn_features_v3 = knn_v3(ind_train_df)\n",
    "svm_model_v3_result, svm_scaler_v3_result, svm_features_v3 = svm_v3(ind_train_df)\n",
    "\n",
    "y_test = ind_test_df[target]\n",
    "\n",
    "X_test_lr = ind_test_df[lr_features_v3]\n",
    "X_test_lr_scaled = lr_scaler_v3_result.transform(X_test_lr)\n",
    "\n",
    "X_test_rf = ind_test_df[rf_features_v3]\n",
    "X_test_rf_scaled = rf_scaler_v3_result.transform(X_test_rf)\n",
    "\n",
    "X_test_xgb = ind_test_df[xgb_features_v3]\n",
    "X_test_xgb_scaled = xgb_scaler_v3_result.transform(X_test_xgb)\n",
    "\n",
    "X_test_knn = ind_test_df[knn_features_v3]\n",
    "X_test_knn_scaled = knn_scaler_v3_result.transform(X_test_knn)\n",
    "\n",
    "X_test_svm = ind_test_df[svm_features_v3]\n",
    "X_test_svm_scaled = svm_scaler_v3_result.transform(X_test_svm)\n",
    "\n",
    "y_test_xgb = y_test.map({-1: 0, 1: 1})\n",
    "\n",
    "lr_evaluation_v3 = model_evaluation(lr_model_v3_result, lr_features_v3, target, X_test_lr_scaled, y_test)\n",
    "rf_evaluation_v3 = model_evaluation(rf_model_v3_result, rf_features_v3, target, X_test_rf_scaled, y_test)\n",
    "xgb_evaluation_v3 = model_evaluation(xgb_model_v3_result, xgb_features_v3, target, X_test_xgb_scaled, y_test_xgb)\n",
    "knn_evaluation_v3 = model_evaluation(knn_model_v3_result, knn_features_v3, target, X_test_knn_scaled, y_test)\n",
    "svm_evaluation_v3 = model_evaluation(svm_model_v3_result, svm_features_v3, target, X_test_svm_scaled, y_test)\n",
    "\n",
    "results_df_v3, result_features_v3 = show_model_results(lr_evaluation_v3, rf_evaluation_v3, \n",
    "                                                       xgb_evaluation_v3, knn_evaluation_v3, \n",
    "                                                       svm_evaluation_v3)\n",
    "\n",
    "results_df_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815978f",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc461e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'precision_sell', 'precision_buy', 'recall_sell', \n",
    "           'recall_buy', 'f1_sell', 'f1_buy', 'roc_auc']\n",
    "\n",
    "models = results_df_v1.index.tolist()\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(14, 4 * len(metrics)), sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(x - width, results_df_v1[metric], width, label='V1')\n",
    "    axes[i].bar(x, results_df_v2[metric], width, label='V2')\n",
    "    axes[i].bar(x + width, results_df_v3[metric], width, label='V3')\n",
    "    axes[i].set_title(metric)\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(models, rotation=20)\n",
    "    axes[i].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b7234",
   "metadata": {},
   "source": [
    "# 5. Picking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd17177",
   "metadata": {},
   "source": [
    "ทดสอบกับหลายๆ เหรียญ แล้วหา Model ที่ดีที่สุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_ind_df(symbol):\n",
    "    api = BinanceAPI(api_key, api_secret)\n",
    "    train_df = collect_historical_data(api, symbol, interval=\"5m\", days=4)\n",
    "    test_df = collect_historical_data(api, symbol, interval=\"5m\", days=1)\n",
    "    test_df = test_df[test_df['timestamp'] > train_df['timestamp'].max()]\n",
    "    \n",
    "    ind_train_df = add_all_indicators(train_df)\n",
    "    ind_train_df.set_index(\"timestamp\", inplace=True)\n",
    "    ind_train_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)\n",
    "    ind_train_df['price_direction'] = np.where(\n",
    "        ind_train_df['close'].shift(-1) < ind_train_df['close'], -1, 1\n",
    "    )\n",
    "    \n",
    "    ind_test_df = add_all_indicators(test_df)\n",
    "    ind_test_df.set_index(\"timestamp\", inplace=True)\n",
    "    ind_test_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)\n",
    "    ind_test_df['price_direction'] = np.where(\n",
    "        ind_test_df['close'].shift(-1) < ind_test_df['close'], -1, 1\n",
    "    )\n",
    "    \n",
    "    return ind_train_df, ind_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc08dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(eval_df):\n",
    "    valid_models = eval_df[\n",
    "        (eval_df['prediction_diversity'] >= 2) &\n",
    "        (eval_df['min_pred_ratio'] >= 0.05) &\n",
    "        (eval_df['accuracy'] > 0.45) &\n",
    "        (eval_df['roc_auc'] > 0.5)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(valid_models) == 0:\n",
    "        print(\"\\n--- No models meet the first criteria! ---\")\n",
    "        \n",
    "        relaxed_models = eval_df[\n",
    "            (eval_df['prediction_diversity'] >= 2) &\n",
    "            (eval_df['min_pred_ratio'] >= 0.01)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(relaxed_models) == 0:\n",
    "            print(\"\\nNo Model with prediction_diversity >= 2. So selecting model with highest accuracy...\")\n",
    "            return eval_df['accuracy'].idxmax()\n",
    "        else:\n",
    "            valid_models = relaxed_models\n",
    "    \n",
    "    print(\"\\n=== Calculating Composite Scores ===\")\n",
    "    weights = {\n",
    "        'accuracy': 0.15,           \n",
    "        'balanced_f1': 0.30,        \n",
    "        'balanced_recall': 0.25,    \n",
    "        'roc_auc': 0.10,           \n",
    "        'prediction_stability': 0.20\n",
    "    }\n",
    "    \n",
    "    for idx, row in valid_models.iterrows():\n",
    "        balanced_f1 = (row['f1_sell'] + row['f1_buy']) / 2\n",
    "        balanced_recall = (row['recall_sell'] + row['recall_buy']) / 2\n",
    "        \n",
    "        prediction_stability = 1 - abs(row['min_pred_ratio'] - 0.5)\n",
    "        \n",
    "        composite_score = (\n",
    "            weights['accuracy'] * row['accuracy'] +\n",
    "            weights['balanced_f1'] * balanced_f1 +\n",
    "            weights['balanced_recall'] * balanced_recall +\n",
    "            weights['roc_auc'] * row['roc_auc'] +\n",
    "            weights['prediction_stability'] * prediction_stability\n",
    "        )\n",
    "        \n",
    "        valid_models.loc[idx, 'balanced_f1'] = balanced_f1\n",
    "        valid_models.loc[idx, 'balanced_recall'] = balanced_recall\n",
    "        valid_models.loc[idx, 'prediction_stability'] = prediction_stability\n",
    "        valid_models.loc[idx, 'composite_score'] = composite_score\n",
    "    \n",
    "    best_model_name = valid_models['composite_score'].idxmax()\n",
    "    best_score = valid_models.loc[best_model_name, 'composite_score']\n",
    "    \n",
    "    best_model_row = valid_models.loc[best_model_name]\n",
    "    print(f\"\\n=== Selected Model Details ===\")\n",
    "    print(f\"Best Model: {best_model_name}\")\n",
    "    print(f\"Accuracy: {best_model_row['accuracy']:.4f}\")\n",
    "    print(f\"Prediction Diversity: {best_model_row['prediction_diversity']}\")\n",
    "    print(f\"Min Prediction Ratio: {best_model_row['min_pred_ratio']:.4f}\")\n",
    "    print(f\"ROC AUC: {best_model_row['roc_auc']:.4f}\")\n",
    "    print(f\"Balanced F1: {best_model_row['balanced_f1']:.4f}\")\n",
    "    print(f\"Balanced Recall: {best_model_row['balanced_recall']:.4f}\")\n",
    "    \n",
    "    return best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85452047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def get_best_model(ind_train_df):\n",
    "    if ind_train_df.shape[0] == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    models = {}\n",
    "    scalers = {}\n",
    "    model_features = {}\n",
    "    evaluations = {}\n",
    "    \n",
    "    print(\"\\n=== Training All Models ===\")\n",
    "    \n",
    "    # V1 Models\n",
    "    lr_model_v1, lr_scaler_v1 = lr_v1(ind_train_df)\n",
    "    models['LR_V1'] = lr_model_v1\n",
    "    scalers['LR_V1'] = lr_scaler_v1\n",
    "    model_features['LR_V1'] = features\n",
    "    \n",
    "    rf_model_v1, rf_scaler_v1 = rf_v1(ind_train_df)\n",
    "    models['RF_V1'] = rf_model_v1\n",
    "    scalers['RF_V1'] = rf_scaler_v1\n",
    "    model_features['RF_V1'] = features\n",
    "    \n",
    "    xgb_model_v1, xgb_scaler_v1 = xgb_v1(ind_train_df)\n",
    "    models['XGB_V1'] = xgb_model_v1\n",
    "    scalers['XGB_V1'] = xgb_scaler_v1\n",
    "    model_features['XGB_V1'] = features\n",
    "    \n",
    "    knn_model_v1, knn_scaler_v1 = knn_v1(ind_train_df)\n",
    "    models['KNN_V1'] = knn_model_v1\n",
    "    scalers['KNN_V1'] = knn_scaler_v1\n",
    "    model_features['KNN_V1'] = features\n",
    "    \n",
    "    svm_model_v1, svm_scaler_v1 = svm_v1(ind_train_df)\n",
    "    models['SVM_V1'] = svm_model_v1\n",
    "    scalers['SVM_V1'] = svm_scaler_v1\n",
    "    model_features['SVM_V1'] = features\n",
    "    \n",
    "    # V2 Models\n",
    "    lr_model_v2, lr_scaler_v2 = lr_v2(ind_train_df)\n",
    "    models['LR_V2'] = lr_model_v2\n",
    "    scalers['LR_V2'] = lr_scaler_v2\n",
    "    model_features['LR_V2'] = features\n",
    "    \n",
    "    rf_model_v2, rf_scaler_v2 = rf_v2(ind_train_df)\n",
    "    models['RF_V2'] = rf_model_v2\n",
    "    scalers['RF_V2'] = rf_scaler_v2\n",
    "    model_features['RF_V2'] = features\n",
    "    \n",
    "    xgb_model_v2, xgb_scaler_v2 = xgb_v2(ind_train_df)\n",
    "    models['XGB_V2'] = xgb_model_v2\n",
    "    scalers['XGB_V2'] = xgb_scaler_v2\n",
    "    model_features['XGB_V2'] = features\n",
    "    \n",
    "    knn_model_v2, knn_scaler_v2 = knn_v2(ind_train_df)\n",
    "    models['KNN_V2'] = knn_model_v2\n",
    "    scalers['KNN_V2'] = knn_scaler_v2\n",
    "    model_features['KNN_V2'] = features\n",
    "    \n",
    "    svm_model_v2, svm_scaler_v2 = svm_v2(ind_train_df)\n",
    "    models['SVM_V2'] = svm_model_v2\n",
    "    scalers['SVM_V2'] = svm_scaler_v2\n",
    "    model_features['SVM_V2'] = features\n",
    "    \n",
    "    # V3 Models\n",
    "    lr_model_v3, lr_scaler_v3, lr_features_v3 = lr_v3(ind_train_df)\n",
    "    models['LR_V3'] = lr_model_v3\n",
    "    scalers['LR_V3'] = lr_scaler_v3\n",
    "    model_features['LR_V3'] = lr_features_v3\n",
    "    \n",
    "    rf_model_v3, rf_scaler_v3, rf_features_v3 = rf_v3(ind_train_df)\n",
    "    models['RF_V3'] = rf_model_v3\n",
    "    scalers['RF_V3'] = rf_scaler_v3\n",
    "    model_features['RF_V3'] = rf_features_v3\n",
    "    \n",
    "    xgb_model_v3, xgb_scaler_v3, xgb_features_v3 = xgb_v3(ind_train_df)\n",
    "    models['XGB_V3'] = xgb_model_v3\n",
    "    scalers['XGB_V3'] = xgb_scaler_v3\n",
    "    model_features['XGB_V3'] = xgb_features_v3\n",
    "    \n",
    "    knn_model_v3, knn_scaler_v3, knn_features_v3 = knn_v3(ind_train_df)\n",
    "    models['KNN_V3'] = knn_model_v3\n",
    "    scalers['KNN_V3'] = knn_scaler_v3\n",
    "    model_features['KNN_V3'] = knn_features_v3\n",
    "    \n",
    "    svm_model_v3, svm_scaler_v3, svm_features_v3 = svm_v3(ind_train_df)\n",
    "    models['SVM_V3'] = svm_model_v3\n",
    "    scalers['SVM_V3'] = svm_scaler_v3\n",
    "    model_features['SVM_V3'] = svm_features_v3\n",
    "    \n",
    "    print(\"\\n=== Cross-Validation Model Evaluation ===\")\n",
    "    \n",
    "    eval_data = []\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            model_feature_list = model_features[model_name]\n",
    "            X = ind_train_df[model_feature_list]\n",
    "            y = ind_train_df[target]\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            \n",
    "            cv_accuracies = []\n",
    "            cv_predictions = []\n",
    "            cv_true_labels = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                # Scale data\n",
    "                scaler_cv = scalers[model_name].__class__()\n",
    "                X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n",
    "                X_val_cv_scaled = scaler_cv.transform(X_val_cv)\n",
    "                \n",
    "                # Train model\n",
    "                model_cv = models[model_name].__class__(**models[model_name].get_params())\n",
    "                \n",
    "                if 'XGB' in model_name:\n",
    "                    y_train_cv_model = y_train_cv.map({-1: 0, 1: 1})\n",
    "                    y_val_cv_model = y_val_cv.map({-1: 0, 1: 1})\n",
    "                else:\n",
    "                    y_train_cv_model = y_train_cv\n",
    "                    y_val_cv_model = y_val_cv\n",
    "                \n",
    "                model_cv.fit(X_train_cv_scaled, y_train_cv_model)\n",
    "                y_pred_cv = model_cv.predict(X_val_cv_scaled)\n",
    "                \n",
    "                if 'XGB' in model_name:\n",
    "                    y_pred_cv = np.where(y_pred_cv == 0, -1, 1)\n",
    "                    y_val_cv_model = np.where(y_val_cv_model == 0, -1, 1)\n",
    "                \n",
    "                cv_accuracies.append(accuracy_score(y_val_cv_model, y_pred_cv))\n",
    "                cv_predictions.extend(y_pred_cv)\n",
    "                cv_true_labels.extend(y_val_cv_model)\n",
    "            \n",
    "            # คำนวณ metrics จาก CV results\n",
    "            cv_predictions = np.array(cv_predictions)\n",
    "            cv_true_labels = np.array(cv_true_labels)\n",
    "            \n",
    "            unique_preds = np.unique(cv_predictions)\n",
    "            pred_counts = pd.Series(cv_predictions).value_counts(normalize=True)\n",
    "            min_pred_ratio = pred_counts.min()\n",
    "            \n",
    "            avg_accuracy = np.mean(cv_accuracies)\n",
    "            \n",
    "            precision_all = precision_score(cv_true_labels, cv_predictions, average=None, labels=[-1, 1], zero_division=0)\n",
    "            recall_all = recall_score(cv_true_labels, cv_predictions, average=None, labels=[-1, 1], zero_division=0)\n",
    "            f1_all = f1_score(cv_true_labels, cv_predictions, average=None, labels=[-1, 1], zero_division=0)\n",
    "            \n",
    "            precision_sell = precision_all[0] if len(precision_all) > 0 else 0\n",
    "            precision_buy = precision_all[1] if len(precision_all) > 1 else 0\n",
    "            recall_sell = recall_all[0] if len(recall_all) > 0 else 0\n",
    "            recall_buy = recall_all[1] if len(recall_all) > 1 else 0\n",
    "            f1_sell = f1_all[0] if len(f1_all) > 0 else 0\n",
    "            f1_buy = f1_all[1] if len(f1_all) > 1 else 0\n",
    "            \n",
    "            try:\n",
    "                roc_auc = roc_auc_score(cv_true_labels, cv_predictions)\n",
    "            except:\n",
    "                roc_auc = 0.5\n",
    "            \n",
    "            eval_data.append({\n",
    "                'model': model_name,\n",
    "                'accuracy': avg_accuracy,\n",
    "                'prediction_diversity': len(unique_preds),\n",
    "                'min_pred_ratio': min_pred_ratio,\n",
    "                'recall_sell': recall_sell,\n",
    "                'recall_buy': recall_buy,\n",
    "                'f1_sell': f1_sell,\n",
    "                'f1_buy': f1_buy,\n",
    "                'roc_auc': roc_auc\n",
    "            })\n",
    "            \n",
    "            print(f\"{model_name}: CV Accuracy={avg_accuracy:.4f}, \"\n",
    "                  f\"Diversity={len(unique_preds)}, \"\n",
    "                  f\"Min_ratio={min_pred_ratio:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{model_name}: Error - {e}\")\n",
    "            eval_data.append({\n",
    "                'model': model_name,\n",
    "                'accuracy': 0.0,\n",
    "                'prediction_diversity': 0,\n",
    "                'min_pred_ratio': 0.0,\n",
    "                'recall_sell': 0.0,\n",
    "                'recall_buy': 0.0,\n",
    "                'f1_sell': 0.0,\n",
    "                'f1_buy': 0.0,\n",
    "                'roc_auc': 0.5\n",
    "            })\n",
    "    \n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    eval_df.set_index('model', inplace=True)\n",
    "    \n",
    "    print(\"\\n=== Model Selection ===\")\n",
    "    best_model_name = select_best_model(eval_df)\n",
    "    \n",
    "    if best_model_name not in models:\n",
    "        print(f\"Selected model {best_model_name} not found, falling back to highest accuracy\")\n",
    "        best_model_name = eval_df['accuracy'].idxmax()\n",
    "    \n",
    "    best_model = models[best_model_name]\n",
    "    best_scaler = scalers[best_model_name]\n",
    "    best_features = model_features[best_model_name]\n",
    "    \n",
    "    return best_model, best_scaler, best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ecdbc",
   "metadata": {},
   "source": [
    "# 6. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(symbol=None):\n",
    "    api = BinanceAPI(api_key, api_secret)\n",
    "    \n",
    "    while not symbol:\n",
    "        print(\"Available symbols (Some might not work...):\")\n",
    "        for s in api.get_n_symbol(10):\n",
    "            print(s)\n",
    "    \n",
    "        symbol = input(\"Enter a symbol: \").strip().upper()\n",
    "        print(f\"\\nSelected symbol: {symbol}\")\n",
    "\n",
    "        ind_train_df, ind_test_df = create_train_test_ind_df(symbol)\n",
    "        if ind_train_df.shape[0] == 0 or ind_test_df.shape[0] == 0:\n",
    "            print(f\"{symbol} is invalid or has insufficient data. Try again\")\n",
    "            symbol = None\n",
    "            \n",
    "    ind_train_df, ind_test_df = create_train_test_ind_df(symbol)\n",
    "    print(f\"\\nTrain set size: {ind_train_df.shape}, Test set size: {ind_test_df.shape}\")\n",
    "    \n",
    "    print(\"\\nTrain target distribution:\")\n",
    "    print(ind_train_df['price_direction'].value_counts(normalize=True))\n",
    "    print(\"\\nTest target distribution:\")\n",
    "    print(ind_test_df['price_direction'].value_counts(normalize=True))\n",
    "\n",
    "    #  Train with Train data\n",
    "    best_model, best_scaler, best_features = get_best_model(ind_train_df)\n",
    "    \n",
    "    if best_model is None:\n",
    "        print(\"No valid model!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Best model: {type(best_model).__name__}\")\n",
    "    print(f\"Features used: {best_features}\")\n",
    "\n",
    "    # Test with Test data\n",
    "    print(\"\\n=== Testing on Real Test Data ===\")\n",
    "    x_test = ind_test_df[best_features]\n",
    "    y_test = ind_test_df['price_direction']\n",
    "    \n",
    "    x_test_scaled = best_scaler.transform(x_test)\n",
    "    y_pred = best_model.predict(x_test_scaled)\n",
    "    \n",
    "    if hasattr(best_model, 'predict') and 'XGB' in str(type(best_model)):\n",
    "        y_pred = np.where(y_pred == 0, -1, 1)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\n=== Test Results ===\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"📊 Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    unique_preds = np.unique(y_pred)\n",
    "    pred_counts = pd.Series(y_pred).value_counts(normalize=True)\n",
    "    print(f\"Prediction diversity: {len(unique_preds)} classes\")\n",
    "    print(f\"Prediction distribution:\")\n",
    "    for pred, count in pred_counts.items():\n",
    "        print(f\"Class {pred}: {count:.2%}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Trading Simulation\n",
    "    print(\"\\n=== Trading Strategy Simulation ===\")\n",
    "    initial_balance = 1000\n",
    "    balance = initial_balance\n",
    "    position = 0\n",
    "    trades = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    trade_history = []\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        predicted_direction = y_pred[i]\n",
    "        actual_direction = y_test.iloc[i]\n",
    "        \n",
    "        if predicted_direction == actual_direction:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        if predicted_direction == 1 and position <= 0:  # Buy signal\n",
    "            if position == -1: \n",
    "                trades += 1\n",
    "                if actual_direction == 1:\n",
    "                    balance -= 10\n",
    "                else:  \n",
    "                    balance += 10\n",
    "            \n",
    "            position = 1\n",
    "            trades += 1\n",
    "            \n",
    "        elif predicted_direction == -1 and position >= 0:  # Sell signal\n",
    "            if position == 1:\n",
    "                trades += 1\n",
    "                if actual_direction == 1:\n",
    "                    balance += 10\n",
    "                else: \n",
    "                    balance -= 10\n",
    "            \n",
    "            position = -1 \n",
    "            trades += 1\n",
    "    \n",
    "    if position != 0:\n",
    "        trades += 1\n",
    "        if len(y_test) > 0:\n",
    "            final_actual = y_test.iloc[-1]\n",
    "            if (position == 1 and final_actual == 1) or (position == -1 and final_actual == -1):\n",
    "                balance += 10\n",
    "            else:\n",
    "                balance -= 10\n",
    "    \n",
    "    total_return = (balance - initial_balance) / initial_balance * 100\n",
    "    prediction_accuracy = (correct_predictions / len(y_pred) * 100) if len(y_pred) > 0 else 0\n",
    "    \n",
    "    print(f\"💵 Initial balance: ${initial_balance}\")\n",
    "    print(f\"💰 Final balance: ${balance:.2f}\")\n",
    "    print(f\"📈 Total return: {total_return:.2f}%\")\n",
    "    print(f\"🎯 Prediction accuracy: {prediction_accuracy:.2f}%\")\n",
    "    print(f\"📊 Total trades: {trades}\")\n",
    "    \n",
    "    # ✅ Additional Analysis\n",
    "    print(f\"\\n=== Additional Analysis ===\")\n",
    "    print(f\"🔮 Total predictions: {len(y_pred)}\")\n",
    "    print(f\"✅ Correct predictions: {correct_predictions}\")\n",
    "    print(f\"❌ Wrong predictions: {len(y_pred) - correct_predictions}\")\n",
    "    \n",
    "    if len(unique_preds) < 2:\n",
    "        print(f\"⚠️ WARNING: Model only predicts {len(unique_preds)} class(es)\")\n",
    "        print(f\"🔄 This indicates potential overfitting or poor model selection\")\n",
    "        print(f\"📊 Model prediction distribution: {dict(pred_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate(\"ETHBTC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
