{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0c8d25",
   "metadata": {},
   "source": [
    "# 0. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd754d",
   "metadata": {},
   "source": [
    "## 0.1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install python-binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"API_KEY\") if os.environ.get(\"API_KEY\") else \"\"\n",
    "api_secret = os.environ.get(\"API_SECRET\") if os.environ.get(\"API_SECRET\") else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877efb33",
   "metadata": {},
   "source": [
    "## 0.2. Connecting API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60663edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.binance.com\"\n",
    "api_call = \"/api/v3/ticker/price\"\n",
    "headers = {\"content-type\": \"application/json\", \"X-MBX-APIKEY\": api_key}\n",
    "\n",
    "response = requests.get(url + api_call, headers=headers)\n",
    "response = json.loads(response.text)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe786c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(response)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff6833",
   "metadata": {},
   "source": [
    "# 1. Binance API\n",
    "\n",
    "```text\n",
    "Documentation: https://developers.binance.com/docs/binance-spot-api-docs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinanceAPI:\n",
    "    def __init__(self, api_key=None, api_secret=None):\n",
    "        self.base_url = \"https://api.binance.com\"\n",
    "        self.api_key = api_key\n",
    "        self.api_secret = api_secret\n",
    "        \n",
    "    # ข้อมูลเทียนหรือกราฟแท่งเทียนในอดีตสำหรับคู่การเทรดที่กำหนดตาม Symbol\n",
    "    def get_klines(self, symbol, interval, limit=1000, start_time=None, end_time=None):\n",
    "        endpoint = \"/api/v3/klines\"\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'limit': limit\n",
    "        }\n",
    "        \n",
    "        if start_time:\n",
    "            params['startTime'] = start_time\n",
    "        if end_time:\n",
    "            params['endTime'] = end_time\n",
    "            \n",
    "        response = requests.get(self.base_url + endpoint, params=params)\n",
    "        return response.json()\n",
    "    \n",
    "    def get_n_symbol(self, n) :\n",
    "        endpoint = \"/api/v3/ticker/price\"\n",
    "        headers = {\"content-type\": \"application/json\", \"X-MBX-APIKEY\": self.api_key}\n",
    "        response = requests.get(self.base_url + endpoint, headers=headers)\n",
    "        response = json.loads(response.text)\n",
    "        df = pd.DataFrame.from_records(response)\n",
    "        return df.loc[:n, \"symbol\"] \n",
    "    \n",
    "    def get_server_time(self, as_timestamp=False) :\n",
    "        endpoint = \"/api/v3/time\"\n",
    "        response = requests.get(self.base_url + endpoint)\n",
    "        ts = response.json()[\"serverTime\"]\n",
    "        if as_timestamp:\n",
    "            return ts\n",
    "        time = datetime.fromtimestamp(ts / 1000)\n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # สถิติการเปลี่ยนแปลงราคา 24 ชั่วโมงสำหรับคู่การเทรดที่กำหนดตาม Symbol\n",
    "    def get_24hr_ticker(self, symbol):\n",
    "        endpoint = \"/api/v3/ticker/24hr\"\n",
    "        params = {'symbol': symbol}\n",
    "        response = requests.get(self.base_url + endpoint, params=params)\n",
    "        return response.json()\n",
    "    \n",
    "    # ข้อมูล order book ปัจจุบันสำหรับคู่การเทรดที่กำหนดตาม Symbol\n",
    "    def get_orderbook(self, symbol, limit=100):\n",
    "        endpoint = \"/api/v3/depth\"\n",
    "        params = {'symbol': symbol, 'limit': limit}\n",
    "        response = requests.get(self.base_url + endpoint, params=params)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BinanceAPI(api_key, api_secret)\n",
    "api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.get_server_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.get_n_symbol(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.get_klines(\"ETHBTC\", \"1m\", limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12af73b",
   "metadata": {},
   "source": [
    "# 2. Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_historical_data(api: BinanceAPI, symbol, interval='1m', days=1, end_time=None):\n",
    "    if end_time is None:\n",
    "        end_time = int(api.get_server_time(as_timestamp=True))\n",
    "    start_time = end_time - (days * 24 * 60 * 60 * 1000)\n",
    "    \n",
    "    klines = api.get_klines(\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        'timestamp', 'open', 'high', 'low', 'close', 'volume',\n",
    "        'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "    ])\n",
    "    \n",
    "    # Convert data types\n",
    "    numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')\n",
    "    df.drop([\"ignore\"], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa310ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BinanceAPI(api_key, api_secret)\n",
    "symbol = \"BNBBTC\"\n",
    "\n",
    "train_df = collect_historical_data(api, symbol, interval=\"5m\", days=4)\n",
    "\n",
    "test_df = collect_historical_data(api, symbol, interval=\"1m\", days=1, end_time=None)\n",
    "test_df = test_df[test_df['timestamp'] > train_df['timestamp'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa575d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedcb98",
   "metadata": {},
   "source": [
    "<b>Columns</b>\n",
    "\n",
    "1) <b>open</b>: ราคา *แรกสุด* ที่มีการซื้อขายในช่วงเวลา t\n",
    "2) <b>high</b>: ราคา *สูงสุด* ที่มีการซื้อขายในช่วงเวลา t\n",
    "3) <b>low</b>: ราคา *ต่ำสุด* ที่มีการซื้อขายในช่วงเวลา t\n",
    "4) <b>close</b>: ราคา *สุดท้าย* ที่มีการซื้อขายในช่วงเวลา t\n",
    "\n",
    "``` \n",
    "4 Columns นี้มีการพิจารณาค่าตัวเลขเหมือนกัน เข่น 0.2389 คือ 1 ETH แลกได้ 0.2389 BTC \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "5. <b>volume</b>: จำนวนเหรียญหลักรวมที่มีการซื้อขายในช่วงเวลา t\n",
    "6. <b>quote_asset_volume</b>: จำนวนเหรียญคู่รวมที่มีการซื้อขายในช่วงเวลา t BTC รวม\n",
    "7. <b>number_of_trades</b>: จำนวนครั้งที่มีการซื้อขายในช่วงเวลา t\n",
    "8. <b>taker_buy_base_asset_volume</b>: จำนวนเหรียญหลักรวมที่มีการรีบซื้อในทันทีในช่วงเวลา t\n",
    "9. <b>taker_buy_quote_asset_volume</b>: จำนวนเหรียญคู่รวมที่มีการรีบซื้อในทันทีในช่วงเวลา t\n",
    "\n",
    "---\n",
    "\n",
    "10. <b>timestamp</b>: เวลาเริ่มต้นของการซื้อขาย\n",
    "11. <b>close_time</b>: เวลาสิ้นสุดของการซื้อขาย\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4993b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01450077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"taker_buy_base_asset_volume\"] = train_df[\"taker_buy_base_asset_volume\"].astype(float)\n",
    "train_df[\"taker_buy_quote_asset_volume\"] = train_df[\"taker_buy_quote_asset_volume\"].astype(float)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"taker_buy_base_asset_volume\"] = test_df[\"taker_buy_base_asset_volume\"].astype(float)\n",
    "test_df[\"taker_buy_quote_asset_volume\"] = test_df[\"taker_buy_quote_asset_volume\"].astype(float)\n",
    "\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f044f9",
   "metadata": {},
   "source": [
    "# 3. Create Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458984f",
   "metadata": {},
   "source": [
    "## 3.1. Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e1979",
   "metadata": {},
   "source": [
    "คำนวณค่าเฉลี่ยแบบเคลื่อนที่ทุกๆ n จุด แล้วดูแนวโน้มค่าเฉลี่ยเหล่านั้น"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dad8ce",
   "metadata": {},
   "source": [
    "- Bullish: มีแนวโน้มว่าในอนาคต ราคาสูงขึ้น -> 1 ETH มีแนวโน้มจะได้ BTC มากขึ้น\n",
    "    - ถ้าเรามี ETH อยู่ เราควรถือไว้ หรือซื้อ ETH เพิ่มเติม\n",
    "    - ถ้าเรามี BTC อยู่ เราควรขายเพื่อซื้อ ETH\n",
    "     \n",
    "- Bearish: มีแนวโน้มว่าในอนาคต ราคาลดลง -> 1 ETH มีแนวโน้มจะได้ BTC น้อยลง\n",
    "    - ถ้าเรามี ETH อยู่ เราควรขายเพื่อซื้อ BTC\n",
    "    - ถ้าเรามี BTC อยู่ เราควรถือไว้ หรือซื้อ BTC เพิ่มเติม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_moving_average(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    \n",
    "    price_col = \"close\"\n",
    "    \n",
    "    # SMA\n",
    "    for period in [20, 50, 200]:\n",
    "        df[f'SMA_{period}'] = df[price_col].rolling(period, min_periods=1).mean()\n",
    "    \n",
    "    # EMA   \n",
    "    for period in [12, 26]:\n",
    "        df[f'EMA_{period}'] = df[price_col].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # Golden Cross: 50-day SMA crosses above 200-day SMA (Long-term Bullish)\n",
    "    df['golden_cross'] = ((df['SMA_50'] > df['SMA_200']) & \n",
    "                         (df['SMA_50'].shift(1) <= df['SMA_200'].shift(1))).astype(int)\n",
    "    \n",
    "    # Death Cross: 50-day SMA crosses below 200-day SMA (Long-term Bearish)\n",
    "    df['death_cross'] = ((df['SMA_50'] < df['SMA_200']) & \n",
    "                        (df['SMA_50'].shift(1) >= df['SMA_200'].shift(1))).astype(int)\n",
    "    \n",
    "    # Bullish Cross: 20-day SMA crosses above 50-day SMA (Short-term Bullish)\n",
    "    df['bullish_cross'] = ((df['SMA_20'] > df['SMA_50']) & \n",
    "                          (df['SMA_20'].shift(1) <= df['SMA_50'].shift(1))).astype(int)\n",
    "    \n",
    "    # Bearish Cross: 20-day SMA crosses below 50-day SMA (Short-term Bearish)\n",
    "    df['bearish_cross'] = ((df['SMA_20'] < df['SMA_50']) & \n",
    "                          (df['SMA_20'].shift(1) >= df['SMA_50'].shift(1))).astype(int)\n",
    "    \n",
    "    # EMA Bullish Cross: 12-day EMA crosses above 26-day EMA (Momentum turning up)\n",
    "    df['ema_bullish_cross'] = ((df['EMA_12'] > df['EMA_26']) & \n",
    "                              (df['EMA_12'].shift(1) <= df['EMA_26'].shift(1))).astype(int)\n",
    "    \n",
    "    # EMA Bearish Cross: 12-day EMA crosses below 26-day EMA (Momentum turning down)\n",
    "    df['ema_bearish_cross'] = ((df['EMA_12'] < df['EMA_26']) & \n",
    "                              (df['EMA_12'].shift(1) >= df['EMA_26'].shift(1))).astype(int)\n",
    "    \n",
    "    # 0 = Very Bearish, 5 = Very Bullish\n",
    "    df['trend_strength'] = ((df[price_col] > df['SMA_20']).astype(int) + \n",
    "                           (df[price_col] > df['SMA_50']).astype(int) + \n",
    "                           (df[price_col] > df['SMA_200']).astype(int) +\n",
    "                           (df[price_col] > df['EMA_12']).astype(int) +\n",
    "                           (df[price_col] > df['EMA_26']).astype(int))\n",
    "    \n",
    "    # Price Distance from MAs\n",
    "    # Positive = Above MA \n",
    "    # Negative = Below MA\n",
    "    df['price_sma20_dist'] = ((df[price_col] - df['SMA_20']) / df['SMA_20']).fillna(0)\n",
    "    df['price_sma50_dist'] = ((df[price_col] - df['SMA_50']) / df['SMA_50']).fillna(0)\n",
    "    df['price_sma200_dist'] = ((df[price_col] - df['SMA_200']) / df['SMA_200']).fillna(0)\n",
    "    \n",
    "    # Price Distance from EMAs\n",
    "    # Positive = Above MA \n",
    "    # Negative = Below MA\n",
    "    df['price_ema12_dist'] = ((df[price_col] - df['EMA_12']) / df['EMA_12']).fillna(0)\n",
    "    df['price_ema26_dist'] = ((df[price_col] - df['EMA_26']) / df['EMA_26']).fillna(0)\n",
    "    \n",
    "    # Bullish Alignment: SMA_20 > SMA_50 > SMA_200\n",
    "    df['bullish_alignment'] = ((df['SMA_20'] > df['SMA_50']) & \n",
    "                              (df['SMA_50'] > df['SMA_200'])).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb21f21",
   "metadata": {},
   "source": [
    "## 3.2. Relative Strength Index (RSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790ef08",
   "metadata": {},
   "source": [
    "https://www.investopedia.com/terms/r/rsi.asp\n",
    "- บ่งบอกความแรงของราคา (Momentum) ในช่วงเวลาที่กำหนด ซึ่งมักนิยมใช้ 14 วัน\n",
    "- มีค่าอยู่ในช่วงระหว่าง 0 ถึง 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc37c0d",
   "metadata": {},
   "source": [
    "พิจารณา ETHBTC\n",
    "- RSI สูง (>70) = คนซื้อ ETH ด้วย BTC เยอะมาก \n",
    "    - ETH อาจแพงเกินไปที่จะซื้อตอนนี้ \n",
    "    - ตอนนี้เราควรขาย ETH และซื้อ BTC\n",
    "- RSI ต่ำ (<30) = คนขาย ETH เพื่อซื้อ BTC เยอะมาก \n",
    "    - ETH อาจถูกเกินไปที่จะขายตอนนี้ \n",
    "    - ตอนนี้เราควรซื้อ ETH และขาย BTC\n",
    "- RSI กลางๆ (~50) = การซื้อขาย ETH/BTC ปกติดี  \n",
    "    - ตอนนี้ควรรอดูสถานการณ์ก่อน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rsi(df:pd.DataFrame, period=14):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    price_col='close'\n",
    "    \n",
    "    delta = df[price_col].diff()\n",
    "    \n",
    "    gains = delta.where(delta > 0, 0)\n",
    "    losses = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    # Average Gain and Loss with Exponential Moving Average\n",
    "    avg_gains = gains.ewm(alpha=1/period, adjust=False).mean()\n",
    "    avg_losses = losses.ewm(alpha=1/period, adjust=False).mean()\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rs = avg_gains / avg_losses\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['rsi'] = rsi\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b48b7d",
   "metadata": {},
   "source": [
    "## 3.3. MACD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93bffa",
   "metadata": {},
   "source": [
    "- ใช้ดูแนวโน้ม (trend) และโมเมนตัม (momentum) ของราคา \n",
    "- ดูจากความแตกต่างของค่าเฉลี่ยเคลื่อนที่แบบ EMA สองเส้น (fast: EMA12 ลบกับ slow: EMA26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5c724",
   "metadata": {},
   "source": [
    "พิจารณา ETHBTC\n",
    "- MACD > 0 แสดงว่าราคากำลังขึ้น หรือก็คือมีแนวโน้มว่าในอนาคต BTC จะมีราคาสูงขึ้น\n",
    "    - ถ้าเราถือ ETH เราควรถือไว้ รอขายในอนาคต\n",
    "    - ถ้าเราถือ BTC เราควรขายเพื่อซื้อ ETH\n",
    "- MACD < 0 แสดงว่าราคากำลังลง หรือก็คือมีแนวโน้มว่าในอนาคต BTC จะมีราคาต่ำขึ้น\n",
    "    - ถ้าเราถือ ETH เราควรขายเพื่อซื้อ BTC\n",
    "    - ถ้าเราถือ BTC เราควรถือไว้ รอขายในอนาคต\n",
    "\n",
    "** ยิ่งค่าห่างจาก 0 ยิ่งมีแนวโน้มที่จะไปทางนั้นๆ สูง\n",
    "\n",
    "- จังหวะที่ควรซื้อ หรือขาย คือจังหวะที่เส้นของ MACD ตัดกับเส้น MACD_Signal\n",
    "    - MACD ตัดแล้วขึ้นสูงกว่า MACD_Signal: เป็นช่วงราคากำลังขึ้น\n",
    "    - MACD ตัดแล้วต่ำกว่า MACD_Signal: เป็นช่วงราคากำลังลง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5832f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_macd(df, fast=12, slow=26, signal=9):\n",
    "    # Calculate EMA fast and slow\n",
    "    df['ema_fast'] = df['close'].ewm(span=fast, adjust=False).mean()\n",
    "    df['ema_slow'] = df['close'].ewm(span=slow, adjust=False).mean()\n",
    "    \n",
    "    # MACD line\n",
    "    df['macd'] = df['ema_fast'] - df['ema_slow']\n",
    "    \n",
    "    # Signal line\n",
    "    df['macd_signal'] = df['macd'].ewm(span=signal, adjust=False).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce99a58",
   "metadata": {},
   "source": [
    "## 3.4. Bollinger Bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c45401",
   "metadata": {},
   "source": [
    "- ประกอบด้วย 3 เส้นหลัก:\n",
    "\n",
    "1. เส้นกลาง (Middle Band) : SMA ของ close หมายถึงแนวโน้มราคากลาง ๆ ในช่วงเวลาที่กำหนด\n",
    "\n",
    "2. เส้นบน (Upper Band): SMA + 2SD บ่งบอกขอบเขตราคาที่ \"สูงกว่าปกติ\" หรือเป็นระดับแนวต้าน\n",
    "\n",
    "3. เส้นล่าง (Lower Band): SMA - 2SD บ่งบอกขอบเขตราคาที่ \"ต่ำกว่าปกติ\" หรือเป็นระดับแนวรับ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516ee52",
   "metadata": {},
   "source": [
    "พิจารณา ETHBTC\n",
    "- มี 4 Case ที่เกิดขึ้นได้\n",
    "    1. Close Price ชิด Upper Band: ราคาสูงเกินไปแล้ว อาจมีโอกาสราคาตกลงในเร็ว ๆ นี้\n",
    "        - ถ้าเราถือ ETH ควรขายเพื่อซื้อ BTC\n",
    "        - ถ้าเราถือ BTC ควรถือไว้ รอขายในอนาคต\n",
    "    2. Close Price ชิด Lower Band: ราคาต่ำเกินไปแล้ว อาจมีโอกาสราคาขึ้นในเร็ว ๆ นี้\n",
    "        - ถ้าเราถือ ETH ควรถือไว้ รอขายในอนาคต\n",
    "        - ถ้าเราถือ BTC ควรขายเพื่อซื้อ ETH\n",
    "    3. Upper Band กับ Lower Band เข้ามาชิดกัน: ความผันผวนต่ำ เตรียมเคลื่อนไหว\n",
    "        - ถ้า Close Price สูงกว่า Upper Band อาจเป็นสัญญาณซื้อ ETH ขาย BTC\n",
    "        - ถ้า Close Price ต่ำกว่า Lower Band อาจเป็นสัญญาณขาย ETH ซื้อ BTC\n",
    "    4. Upper Band กับ Lower Band ห่างออกจากกัน: ความผันผวนสูง\n",
    "        - ถ้า Close Price สูงขึ้น และอยู่ใกล้ Upper Band ควรถือ ETH ต่อเนื่อง หรือซื้อเพิ่ม\n",
    "        - ถ้า Close Price ลดลง และอยู่ใกล้ Lower Band → ควรขาย ETH ซื้อ BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566235de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bollinger(df, period=20, std_dev=2):\n",
    "    df['bb_middle'] = df['close'].rolling(window=period).mean()\n",
    "    df['bb_std'] = df['close'].rolling(window=period).std()\n",
    "    \n",
    "    df['bb_upper'] = df['bb_middle'] + std_dev * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_middle'] - std_dev * df['bb_std']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_indicators(df) :\n",
    "    df = add_moving_average(df)\n",
    "    df = add_rsi(df)\n",
    "    df = add_macd(df)\n",
    "    df = add_bollinger(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef472f",
   "metadata": {},
   "source": [
    "# 4. Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c1c3d",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df = add_all_indicators(train_df)\n",
    "ind_train_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "ind_test_df = add_all_indicators(test_df)\n",
    "ind_test_df.set_index(\"timestamp\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea907810",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bb_middle\\n\", ind_train_df[\"bb_middle\"])\n",
    "print(\"bb_std\\n\", ind_train_df[\"bb_std\"])\n",
    "print(\"bb_upper\\n\", ind_train_df[\"bb_upper\"])\n",
    "print(\"bb_lower\\n\", ind_train_df[\"bb_lower\"])\n",
    "print(\"rsi\\n\", ind_train_df[\"rsi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df['price_direction'] = np.where(\n",
    "    ind_train_df['close'].shift(-1) < ind_train_df['close'], -1, 1\n",
    ")\n",
    "ind_train_df['price_direction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8902798",
   "metadata": {},
   "source": [
    "- -1 คือ ราคาปิดในอนาคต ***น้อยกว่า*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรขาย\n",
    "- 1 คือ ราคาปิดในอนาคต ***มากกว่าหรือเท่ากับ*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรซื้อ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d660850",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bb_middle\\n\", ind_test_df[\"bb_middle\"])\n",
    "print(\"bb_std\\n\", ind_test_df[\"bb_std\"])\n",
    "print(\"bb_upper\\n\", ind_test_df[\"bb_upper\"])\n",
    "print(\"bb_lower\\n\", ind_test_df[\"bb_lower\"])\n",
    "print(\"rsi\\n\", ind_test_df[\"rsi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955944a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_df['price_direction'] = np.where(\n",
    "    ind_test_df['close'].shift(-1) < ind_test_df['close'], -1, 1\n",
    ")\n",
    "ind_test_df['price_direction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba89c5",
   "metadata": {},
   "source": [
    "- -1 คือ ราคาปิดในอนาคต ***น้อยกว่า*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรขาย\n",
    "- 1 คือ ราคาปิดในอนาคต ***มากกว่าหรือเท่ากับ*** ราคาปิดปัจจุบัน -> ในตอนนี้ควรซื้อ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48403ec",
   "metadata": {},
   "source": [
    "## Preparing to Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_df['price_direction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2025\n",
    "test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02876a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "features = [\n",
    "    'SMA_20', 'SMA_50', 'SMA_200',\n",
    "    'EMA_12', 'EMA_26',\n",
    "    'rsi', \n",
    "    'macd', 'macd_signal',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower',\n",
    "    'trend_strength', 'price_sma20_dist', 'price_sma50_dist',\n",
    "    'price_sma200_dist',\n",
    "    'price_ema12_dist', 'price_ema26_dist', 'bullish_alignment'\n",
    "]\n",
    "\n",
    "target = 'price_direction'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be587dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, features, target, X_test, y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if set(np.unique(y_test)).issubset({0, 1}):\n",
    "        y_pred = np.where(y_pred == 0, -1, 1)\n",
    "        y_test = np.where(y_test == 0, -1, 1)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    # print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    precision_all = precision_score(y_test, y_pred, average=None, labels=[-1, 1])\n",
    "    precision_sell = precision_all[0]\n",
    "    precision_buy = precision_all[1]\n",
    "\n",
    "    recall_all = recall_score(y_test, y_pred, average=None, labels=[-1, 1])\n",
    "    recall_sell = recall_all[0]\n",
    "    recall_buy = recall_all[1]\n",
    "\n",
    "    f1_all = f1_score(y_test, y_pred, average=None, labels=[-1, 1])\n",
    "    f1_sell = f1_all[0]\n",
    "    f1_buy = f1_all[1]\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_score = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_score = model.decision_function(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, y_score)\n",
    "    except Exception as e:\n",
    "        roc_auc = None\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"features\": features,\n",
    "        \"target\": target,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_sell\": precision_sell,\n",
    "        \"precision_buy\": precision_buy,  \n",
    "        \"recall_sell\": recall_sell,\n",
    "        \"recall_buy\": recall_buy,\n",
    "        \"f1_sell\": f1_sell,\n",
    "        \"f1_buy\": f1_buy,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_results(lr_evaluation, rf_evaluation, xgb_evaluation, knn_evaluation, svm_evaluation): \n",
    "    data = {\n",
    "        \"model\": [\"LR\", \"RF\", \"XGB\", \"KNN\", \"SVM\"],\n",
    "        \"accuracy\": [lr_evaluation[\"accuracy\"], rf_evaluation[\"accuracy\"], \n",
    "                    xgb_evaluation[\"accuracy\"], knn_evaluation[\"accuracy\"], \n",
    "                    svm_evaluation[\"accuracy\"]],\n",
    "        \"precision_sell\": [lr_evaluation[\"precision_sell\"], rf_evaluation[\"precision_sell\"],\n",
    "                            xgb_evaluation[\"precision_sell\"], knn_evaluation[\"precision_sell\"], \n",
    "                            svm_evaluation[\"precision_sell\"]],\n",
    "        \"precision_buy\": [lr_evaluation[\"precision_buy\"], rf_evaluation[\"precision_buy\"],\n",
    "                            xgb_evaluation[\"precision_buy\"], knn_evaluation[\"precision_buy\"],\n",
    "                            svm_evaluation[\"precision_buy\"]],\n",
    "        \"recall_sell\": [lr_evaluation[\"recall_sell\"], rf_evaluation[\"recall_sell\"],\n",
    "                            xgb_evaluation[\"recall_sell\"], knn_evaluation[\"recall_sell\"],\n",
    "                            svm_evaluation[\"recall_sell\"]],\n",
    "        \"recall_buy\": [lr_evaluation[\"recall_buy\"], rf_evaluation[\"recall_buy\"],\n",
    "                            xgb_evaluation[\"recall_buy\"], knn_evaluation[\"recall_buy\"],\n",
    "                            svm_evaluation[\"recall_buy\"]],\n",
    "        \"f1_sell\": [lr_evaluation[\"f1_sell\"], rf_evaluation[\"f1_sell\"],\n",
    "                        xgb_evaluation[\"f1_sell\"], knn_evaluation[\"f1_sell\"],\n",
    "                        svm_evaluation[\"f1_sell\"]],\n",
    "        \"f1_buy\": [lr_evaluation[\"f1_buy\"], rf_evaluation[\"f1_buy\"],\n",
    "                        xgb_evaluation[\"f1_buy\"], knn_evaluation[\"f1_buy\"],\n",
    "                        svm_evaluation[\"f1_buy\"]],\n",
    "        \"roc_auc\": [lr_evaluation[\"roc_auc\"], rf_evaluation[\"roc_auc\"],\n",
    "                    xgb_evaluation[\"roc_auc\"], knn_evaluation[\"roc_auc\"],\n",
    "                    svm_evaluation[\"roc_auc\"]]\n",
    "    }\n",
    "    \n",
    "    results_df = pd.DataFrame(data)\n",
    "    results_df.set_index(\"model\", inplace=True)\n",
    "    return results_df, {\"LR\": lr_evaluation[\"features\"], \"RF\": rf_evaluation[\"features\"],\n",
    "                        \"XGB\": xgb_evaluation[\"features\"], \"KNN\": knn_evaluation[\"features\"],\n",
    "                        \"SVM\": svm_evaluation[\"features\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b9333",
   "metadata": {},
   "source": [
    "## Model Training V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4224ffd",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01679741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \n",
    "                                                        test_size=test_size, shuffle=False, \n",
    "                                                        random_state=random_state)\n",
    "    model = LogisticRegression(random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Scoring\n",
    "    lr_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return lr_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376c2f7",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \n",
    "                                                        test_size=test_size, shuffle=False, \n",
    "                                                        random_state=random_state)\n",
    "    model = RandomForestClassifier(random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Scoring\n",
    "    rf_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return rf_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec22b04",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab919f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target].map({-1: 0, 1: 1})\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \n",
    "                                                        test_size=test_size, shuffle=False, \n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    model = XGBClassifier(random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Scoring\n",
    "    xgb_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return xgb_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea426b",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1093c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_model_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \n",
    "                                                        test_size=test_size, shuffle=False, \n",
    "                                                        random_state=random_state)\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Scoring\n",
    "    knn_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return knn_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c918593",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad38b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model_v1(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, \n",
    "                                                        shuffle=False, random_state=random_state)\n",
    "    model = SVC(random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Scoring\n",
    "    svm_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return svm_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9a08f",
   "metadata": {},
   "source": [
    "## Model Summary V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_evaluation, lr_model = lr_model_v1(ind_train_df)\n",
    "rf_evaluation, rf_model = rf_model_v1(ind_train_df)\n",
    "xgb_evaluation, xgb_model = xgb_model_v1(ind_train_df)\n",
    "knn_evaluation, knn_model = knn_model_v1(ind_train_df)\n",
    "svm_evaluation, svm_model = svm_model_v1(ind_train_df)\n",
    "\n",
    "results_df_v1, result_features_v1 = show_model_results(lr_evaluation, rf_evaluation, xgb_evaluation, \n",
    "                                                       knn_evaluation, svm_evaluation)\n",
    "\n",
    "results_df_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29465d",
   "metadata": {},
   "source": [
    "## Model Training V2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab675a8",
   "metadata": {},
   "source": [
    "- เพิ่ม Hyperparameter Tuning \n",
    "- เพิ่ม SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29188cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "features = [\n",
    "    'SMA_20', 'SMA_50', 'SMA_200',\n",
    "    'EMA_12', 'EMA_26',\n",
    "    'rsi', \n",
    "    'macd', 'macd_signal',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower',\n",
    "    'trend_strength', 'price_sma20_dist', 'price_sma50_dist',\n",
    "    'price_sma200_dist',\n",
    "    'price_ema12_dist', 'price_ema26_dist', 'bullish_alignment'\n",
    "]\n",
    "\n",
    "target = 'price_direction' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f434b64",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000, random_state=random_state), \n",
    "        param_grid, scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    # print(\"Best Params:\", grid.best_params_)\n",
    "    # print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "    # 5. Train model\n",
    "    model = LogisticRegression(max_iter=1000, **grid.best_params_, random_state=random_state)\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 6. Scoring\n",
    "    lr_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return lr_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be01378",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [4, 8, 16, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(RandomForestClassifier(random_state=random_state), param_grid,\n",
    "                        scoring='accuracy')\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    # print(\"Best Params:\", grid.best_params_)\n",
    "    # print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "    # 5. Train model\n",
    "    model = RandomForestClassifier(random_state=random_state, **grid.best_params_)\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 6. Scoring\n",
    "    rf_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return rf_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92efc840",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target].map({-1: 0, 1: 1})\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(XGBClassifier(eval_metric='logloss', random_state=random_state),\n",
    "        param_grid, scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    # print(\"Best Params:\", grid.best_params_)\n",
    "    # print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "    # 5. Train model\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        random_state=random_state, **grid.best_params_)\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 6. Scoring\n",
    "    xgb_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return xgb_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10684d33",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefe2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_model_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "    }\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='accuracy')\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    # print(\"Best Params:\", grid.best_params_)\n",
    "    # print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "    # 5. Train model\n",
    "    model = KNeighborsClassifier(**grid.best_params_)\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 6. Scoring\n",
    "    knn_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return knn_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb6f1b",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model_v2(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'C': [1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "    grid = GridSearchCV(SVC(), param_grid, scoring='accuracy')\n",
    "    grid.fit(X_train_res, y_train_res)\n",
    "    # print(\"Best Params:\", grid.best_params_)\n",
    "    # print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "    # 5. Train model\n",
    "    model = SVC(**grid.best_params_, random_state=random_state)\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 6. Scoring\n",
    "    svm_evaluation = model_evaluation(model, features, target, X_test, y_test)\n",
    "    return svm_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b558c",
   "metadata": {},
   "source": [
    "## Model Result V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05550555",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_evaluation_v2, lr_model2 = lr_model_v2(ind_train_df)\n",
    "rf_evaluation_v2, rf_model2 = rf_model_v2(ind_train_df)\n",
    "xgb_evaluation_v2, xgb_model2 = xgb_model_v2(ind_train_df)\n",
    "knn_evaluation_v2, knn_model2 = knn_model_v2(ind_train_df)\n",
    "svm_evaluation_v2, svm_model2 = svm_model_v2(ind_train_df)\n",
    "\n",
    "results_df_v2, result_features_v2 = show_model_results(lr_evaluation_v2, \n",
    "                                                       rf_evaluation_v2, xgb_evaluation_v2, \n",
    "                                                       knn_evaluation_v2, svm_evaluation_v2)\n",
    "\n",
    "results_df_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92794e",
   "metadata": {},
   "source": [
    "## Model Training V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70419385",
   "metadata": {},
   "source": [
    "- เพิ่มการทำ Dynamic Features Selection\n",
    "- เพิ่ม Class Weight เพิ้มโอกาสการทาย class -1 เพราะว่าใน Data class 1 มีสัดส่วนมาก เมื่อเทียบกับ class -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'SMA_20', 'SMA_50', 'SMA_200',\n",
    "    'EMA_12', 'EMA_26',\n",
    "    'rsi', \n",
    "    'macd', 'macd_signal',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower',\n",
    "    'trend_strength', 'price_sma20_dist', 'price_sma50_dist',\n",
    "    'price_sma200_dist',\n",
    "    'price_ema12_dist', 'price_ema26_dist', 'bullish_alignment'\n",
    "]\n",
    "\n",
    "target = 'price_direction' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ffdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_by_coef(model, top_n=10):\n",
    "    coef_abs = np.abs(model.coef_[0])\n",
    "    feature_importance = pd.Series(coef_abs, index=features)\n",
    "    top_feature_importance = feature_importance.sort_values(ascending=False).head(top_n)\n",
    "    return top_feature_importance.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_by_importance(model, top_n=10):\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance = pd.Series(importances, index=features)\n",
    "    top_feature_importance = feature_importance.sort_values(ascending=False).head(top_n)\n",
    "    return top_feature_importance.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf17f52",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Fit model ด้วย features ทั้งหมด\n",
    "    model_all = LogisticRegression(max_iter=1000, random_state=random_state, class_weight='balanced')\n",
    "    model_all.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 5. เลือก top N feature\n",
    "    top_features = get_top_features_by_coef(model_all)\n",
    "\n",
    "    # 6. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    X_top_scaled = scaler.fit_transform(X_top)\n",
    "    X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(\n",
    "        X_top_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 7. Balance Data (SMOTE) สำหรับ top feature\n",
    "    counter_top = Counter(y_train_top)\n",
    "    min_class_count_top = min(counter_top.values())\n",
    "    if min_class_count_top <= 1:\n",
    "        X_train_res_top, y_train_res_top = X_train_top, y_train_top\n",
    "    else:\n",
    "        k_neighbors_top = min(5, min_class_count_top - 1)\n",
    "        sm_top = SMOTE(random_state=random_state, k_neighbors=k_neighbors_top)\n",
    "        X_train_res_top, y_train_res_top = sm_top.fit_resample(X_train_top, y_train_top)\n",
    "\n",
    "    # 8. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'class_weight': [{-1: 5, 1: 1}, {-1: 2, 1: 1}, \"balanced\", {-1: 1, 1: 2}, {-1: 1, 1: 5}]\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000, random_state=random_state), \n",
    "        param_grid, scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    # 9. Train model final\n",
    "    model = LogisticRegression(max_iter=1000, **grid.best_params_, random_state=random_state)\n",
    "    model.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    lr_evaluation = model_evaluation(model, top_features, target, X_test_top, y_test_top)\n",
    "    \n",
    "    return lr_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb184b2",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Fit model ด้วย features ทั้งหมด\n",
    "    model_all = RandomForestClassifier(random_state=random_state, class_weight='balanced')\n",
    "    model_all.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 5. เลือก top N feature\n",
    "    top_features = get_top_features_by_importance(model_all)\n",
    "\n",
    "    # 6. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    X_top_scaled = scaler.fit_transform(X_top)\n",
    "    X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(\n",
    "        X_top_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 7. Balance Data (SMOTE) สำหรับ top feature\n",
    "    counter_top = Counter(y_train_top)\n",
    "    min_class_count_top = min(counter_top.values())\n",
    "    if min_class_count_top <= 1:\n",
    "        X_train_res_top, y_train_res_top = X_train_top, y_train_top\n",
    "    else:\n",
    "        k_neighbors_top = min(5, min_class_count_top - 1)\n",
    "        sm_top = SMOTE(random_state=random_state, k_neighbors=k_neighbors_top)\n",
    "        X_train_res_top, y_train_res_top = sm_top.fit_resample(X_train_top, y_train_top)\n",
    "\n",
    "    # 8. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [4, 8, 16, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight': [{-1: 5, 1: 1}, {-1: 2, 1: 1}, \"balanced\", {-1: 1, 1: 2}, {-1: 1, 1: 5}]\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=random_state),\n",
    "        param_grid, scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    # 9. Train model final\n",
    "    model = RandomForestClassifier(random_state=random_state, **grid.best_params_)\n",
    "    model.fit(X_train_res_top, y_train_res_top)\n",
    "    \n",
    "    rf_evaluation = model_evaluation(model, top_features, target, X_test_top, y_test_top)\n",
    "\n",
    "    return rf_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92c363",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target].map({-1: 0, 1: 1})\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Fit model ด้วย features ทั้งหมด\n",
    "    model_all = XGBClassifier(eval_metric='logloss', random_state=random_state)\n",
    "    model_all.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 5. เลือก top N feature (ถ้าเท่ากับ features ทั้งหมด ให้ลด top_n)\n",
    "    top_n = 10\n",
    "    top_features = get_top_features_by_importance(model_all, top_n=top_n)\n",
    "    while len(top_features) == len(features) and top_n > 1:\n",
    "        top_n -= 1\n",
    "        top_features = get_top_features_by_importance(model_all, top_n=top_n)\n",
    "\n",
    "    # 6. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    scaler_top = StandardScaler()\n",
    "    X_top_scaled = scaler_top.fit_transform(X_top)\n",
    "    X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(\n",
    "        X_top_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 7. Balance Data (SMOTE) สำหรับ top feature\n",
    "    counter_top = Counter(y_train_top)\n",
    "    min_class_count_top = min(counter_top.values())\n",
    "    if min_class_count_top <= 1:\n",
    "        X_train_res_top, y_train_res_top = X_train_top, y_train_top\n",
    "    else:\n",
    "        k_neighbors_top = min(5, min_class_count_top - 1)\n",
    "        sm_top = SMOTE(random_state=random_state, k_neighbors=k_neighbors_top)\n",
    "        X_train_res_top, y_train_res_top = sm_top.fit_resample(X_train_top, y_train_top)\n",
    "\n",
    "    # 8. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        \"scale_pos_weight\": [1, 2, 5],\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        XGBClassifier(eval_metric='logloss', random_state=random_state),\n",
    "        param_grid, scoring='accuracy'\n",
    "    )\n",
    "    grid.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    # 9. Train model final\n",
    "    model = XGBClassifier(eval_metric='logloss', random_state=random_state, **grid.best_params_)\n",
    "    model.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    xgb_evaluation = model_evaluation(model, top_features, target, X_test_top, y_test_top)\n",
    "\n",
    "    return xgb_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf029730",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_model_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Fit model ด้วย features ทั้งหมด\n",
    "    from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "    top_n = 10\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(top_n, X_train_res.shape[1]))\n",
    "    selector.fit(X_train_res, y_train_res)\n",
    "    mask = selector.get_support()\n",
    "    top_features = [feature for feature, selected in zip(features, mask) if selected]\n",
    "\n",
    "    # 6. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    X_top_scaled = scaler.fit_transform(X_top)\n",
    "    X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(\n",
    "        X_top_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 7. Balance Data (SMOTE) สำหรับ top feature\n",
    "    counter_top = Counter(y_train_top)\n",
    "    min_class_count_top = min(counter_top.values())\n",
    "    if min_class_count_top <= 1:\n",
    "        X_train_res_top, y_train_res_top = X_train_top, y_train_top\n",
    "    else:\n",
    "        k_neighbors_top = min(5, min_class_count_top - 1)\n",
    "        sm_top = SMOTE(random_state=random_state, k_neighbors=k_neighbors_top)\n",
    "        X_train_res_top, y_train_res_top = sm_top.fit_resample(X_train_top, y_train_top)\n",
    "\n",
    "    # 8. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "        \n",
    "    }\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='accuracy')\n",
    "    grid.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    # 9. Train model final\n",
    "    model = KNeighborsClassifier(**grid.best_params_)\n",
    "    model.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    knn_evaluation = model_evaluation(model, top_features, target, X_test_top, y_test_top)\n",
    "\n",
    "    return knn_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a7584",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model_v3(df):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # 1. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Balance Data (SMOTE with dynamic k_neighbors)\n",
    "    counter = Counter(y_train)\n",
    "    min_class_count = min(counter.values())\n",
    "    if min_class_count <= 1:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    else:\n",
    "        k_neighbors = min(5, min_class_count - 1)\n",
    "        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # 4. Fit model ด้วย features ทั้งหมด\n",
    "    model_all = SVC(kernel='linear', class_weight='balanced', random_state=random_state)\n",
    "    model_all.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # 5. เลือก top N feature (ใช้ absolute coef_)\n",
    "    top_features = get_top_features_by_coef(model_all)\n",
    "\n",
    "    # 6. เตรียมข้อมูลใหม่เฉพาะ top feature\n",
    "    X_top = df[top_features]\n",
    "    X_top_scaled = scaler.fit_transform(X_top)\n",
    "    X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(\n",
    "        X_top_scaled, y, test_size=test_size, shuffle=False, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 7. Balance Data (SMOTE) สำหรับ top feature\n",
    "    counter_top = Counter(y_train_top)\n",
    "    min_class_count_top = min(counter_top.values())\n",
    "    if min_class_count_top <= 1:\n",
    "        X_train_res_top, y_train_res_top = X_train_top, y_train_top\n",
    "    else:\n",
    "        k_neighbors_top = min(5, min_class_count_top - 1)\n",
    "        sm_top = SMOTE(random_state=random_state, k_neighbors=k_neighbors_top)\n",
    "        X_train_res_top, y_train_res_top = sm_top.fit_resample(X_train_top, y_train_top)\n",
    "\n",
    "    # 8. Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'C': [1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': [{-1: 5, 1: 1}, {-1: 2, 1: 1}, \"balanced\", {-1: 1, 1: 2}, {-1: 1, 1: 5}]\n",
    "    }\n",
    "    grid = GridSearchCV(SVC(), param_grid, scoring='accuracy')\n",
    "    grid.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    # 9. Train model final\n",
    "    model = SVC(**grid.best_params_, random_state=random_state)\n",
    "    model.fit(X_train_res_top, y_train_res_top)\n",
    "\n",
    "    svm_evaluation = model_evaluation(model, top_features, target, X_test_top, y_test_top)\n",
    "    return svm_evaluation, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d563d",
   "metadata": {},
   "source": [
    "## Model Result V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044944",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_evaluation_v3, lr_model3 = lr_model_v3(ind_train_df)\n",
    "rf_evaluation_v3, rf_model3 = rf_model_v3(ind_train_df)\n",
    "xgb_evaluation_v3, xgb_model3 = xgb_model_v3(ind_train_df)\n",
    "knn_evaluation_v3, knn_model3 = knn_model_v3(ind_train_df)\n",
    "svm_evaluation_v3, svm_model3 = svm_model_v3(ind_train_df)\n",
    "\n",
    "results_df_v3, result_features_v3 = show_model_results(lr_evaluation_v3, rf_evaluation_v3, \n",
    "                                                       xgb_evaluation_v3, knn_evaluation_v3, \n",
    "                                                       svm_evaluation_v3)\n",
    "\n",
    "results_df_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815978f",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc461e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'precision_sell', 'precision_buy', 'recall_sell', \n",
    "           'recall_buy', 'f1_sell', 'f1_buy', 'roc_auc']\n",
    "\n",
    "models = results_df_v1.index.tolist()\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(14, 4 * len(metrics)), sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(x - width, results_df_v1[metric], width, label='V1')\n",
    "    axes[i].bar(x, results_df_v2[metric], width, label='V2')\n",
    "    axes[i].bar(x + width, results_df_v3[metric], width, label='V3')\n",
    "    axes[i].set_title(metric)\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(models, rotation=20)\n",
    "    axes[i].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b7234",
   "metadata": {},
   "source": [
    "# 5. Picking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd17177",
   "metadata": {},
   "source": [
    "ทดสอบกับหลายๆ เหรียญ แล้วหา Model ที่ดีที่สุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_ind_df(symbol):\n",
    "    api = BinanceAPI(api_key, api_secret)\n",
    "    train_df = collect_historical_data(api, symbol, interval=\"5m\", days=4)\n",
    "    test_df = collect_historical_data(api, symbol, interval=\"1m\", days=1, end_time=None)\n",
    "    test_df = test_df[test_df['timestamp'] > train_df['timestamp'].max()]\n",
    "    \n",
    "    ind_train_df = add_all_indicators(train_df)\n",
    "    ind_train_df.set_index(\"timestamp\", inplace=True)\n",
    "    ind_train_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)\n",
    "    ind_train_df['price_direction'] = np.where(\n",
    "        ind_train_df['close'].shift(-1) < ind_train_df['close'], -1, 1\n",
    "    )\n",
    "    \n",
    "    ind_test_df = add_all_indicators(test_df)\n",
    "    ind_test_df.set_index(\"timestamp\", inplace=True)\n",
    "    ind_test_df.dropna(subset=['bb_lower', 'bb_upper', 'bb_middle', 'bb_std', 'rsi'], inplace=True)\n",
    "    ind_test_df['price_direction'] = np.where(\n",
    "        ind_test_df['close'].shift(-1) < ind_test_df['close'], -1, 1\n",
    "    )\n",
    "    \n",
    "    return ind_train_df, ind_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e333c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_metric_weights(buy_prop):\n",
    "    if buy_prop < 0.3:\n",
    "        return {'f1_buy': 0.5, 'recall_buy': 0.3, 'roc_auc': 0.2}\n",
    "    elif buy_prop > 0.7:\n",
    "        return {'f1_sell': 0.5, 'recall_sell': 0.3, 'roc_auc': 0.2}\n",
    "    else:\n",
    "        return {'f1_sell': 0.25, 'f1_buy': 0.25, 'roc_auc': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a837f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(result_df, train_df, \n",
    "                      imbalance_threshold=0.2, \n",
    "                      severe_imbalance_penalty=0.7, \n",
    "                      mild_imbalance_penalty=0.9):\n",
    "    class_counts = train_df['price_direction'].value_counts(normalize=True).sort_index()\n",
    "    buy_prop = class_counts.get(1, 0.0)\n",
    "    metric_weights = choose_metric_weights(buy_prop)\n",
    "\n",
    "    def score_row(row):\n",
    "        if row['recall_sell'] == 0 or row['recall_buy'] == 0:\n",
    "            return -999\n",
    "        score = sum(row[m] * w for m, w in metric_weights.items())\n",
    "        if buy_prop < imbalance_threshold:\n",
    "            score *= severe_imbalance_penalty\n",
    "        elif buy_prop < 0.3:\n",
    "            score *= mild_imbalance_penalty\n",
    "        return score\n",
    "\n",
    "    scores = result_df.apply(score_row, axis=1)\n",
    "    best_model = scores.idxmax()\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85452047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(ind_train_df):\n",
    "    \n",
    "    if ind_train_df.shape[0] != 0:\n",
    "        # --- V1 ---\n",
    "        lr_eval_v1, lr_model_v1_ = lr_model_v1(ind_train_df)\n",
    "        rf_eval_v1, rf_model_v1_ = rf_model_v1(ind_train_df)\n",
    "        xgb_eval_v1, xgb_model_v1_ = xgb_model_v1(ind_train_df)\n",
    "        knn_eval_v1, knn_model_v1_ = knn_model_v1(ind_train_df)\n",
    "        svm_eval_v1, svm_model_v1_ = svm_model_v1(ind_train_df)\n",
    "\n",
    "        result_df_v1, result_features_v1 = show_model_results(\n",
    "            lr_eval_v1, rf_eval_v1, xgb_eval_v1, knn_eval_v1, svm_eval_v1\n",
    "        )\n",
    "        result_df_v1.index = [f\"{name}_V1\" for name in result_df_v1.index]\n",
    "\n",
    "        # --- V2 ---\n",
    "        lr_eval_v2, lr_model_v2_ = lr_model_v2(ind_train_df)\n",
    "        rf_eval_v2, rf_model_v2_ = rf_model_v2(ind_train_df)\n",
    "        xgb_eval_v2, xgb_model_v2_ = xgb_model_v2(ind_train_df)\n",
    "        knn_eval_v2, knn_model_v2_ = knn_model_v2(ind_train_df)\n",
    "        svm_eval_v2, svm_model_v2_ = svm_model_v2(ind_train_df)\n",
    "\n",
    "        result_df_v2, result_features_v2 = show_model_results(\n",
    "            lr_eval_v2, rf_eval_v2, xgb_eval_v2, knn_eval_v2, svm_eval_v2\n",
    "        )\n",
    "        result_df_v2.index = [f\"{name}_V2\" for name in result_df_v2.index]\n",
    "\n",
    "        # --- V3 ---\n",
    "        lr_eval_v3, lr_model_v3_ = lr_model_v3(ind_train_df)\n",
    "        rf_eval_v3, rf_model_v3_ = rf_model_v3(ind_train_df)\n",
    "        xgb_eval_v3, xgb_model_v3_ = xgb_model_v3(ind_train_df)\n",
    "        knn_eval_v3, knn_model_v3_ = knn_model_v3(ind_train_df)\n",
    "        svm_eval_v3, svm_model_v3_ = svm_model_v3(ind_train_df)\n",
    "\n",
    "        result_df_v3, result_features_v3 = show_model_results(\n",
    "            lr_eval_v3, rf_eval_v3, xgb_eval_v3, knn_eval_v3, svm_eval_v3\n",
    "        )\n",
    "        result_df_v3.index = [f\"{name}_V3\" for name in result_df_v3.index]\n",
    "\n",
    "        result_df = pd.concat([result_df_v1, result_df_v2, result_df_v3])\n",
    "        best_model = select_best_model(result_df, ind_train_df)\n",
    "        \n",
    "        map_model = {\n",
    "            \"LR_V1\": [lr_model_v1_, result_features_v1[\"LR\"]],\n",
    "            \"LR_V2\": [lr_model_v2_, result_features_v2[\"LR\"]],\n",
    "            \"LR_V3\": [lr_model_v3_, result_features_v3[\"LR\"]],\n",
    "            \"RF_V1\": [rf_model_v1_, result_features_v1[\"RF\"]],\n",
    "            \"RF_V2\": [rf_model_v2_, result_features_v2[\"RF\"]],\n",
    "            \"RF_V3\": [rf_model_v3_, result_features_v3[\"RF\"]],\n",
    "            \"XGB_V1\": [xgb_model_v1_, result_features_v1[\"XGB\"]],\n",
    "            \"XGB_V2\": [xgb_model_v2_, result_features_v2[\"XGB\"]],\n",
    "            \"XGB_V3\": [xgb_model_v3_, result_features_v3[\"XGB\"]],\n",
    "            \"KNN_V1\": [knn_model_v1_, result_features_v1[\"KNN\"]],\n",
    "            \"KNN_V2\": [knn_model_v2_, result_features_v2[\"KNN\"]],\n",
    "            \"KNN_V3\": [knn_model_v3_, result_features_v3[\"KNN\"]],\n",
    "            \"SVM_V1\": [svm_model_v1_, result_features_v1[\"SVM\"]],\n",
    "            \"SVM_V2\": [svm_model_v2_, result_features_v2[\"SVM\"]],\n",
    "            \"SVM_V3\": [svm_model_v3_, result_features_v3[\"SVM\"]],\n",
    "        }\n",
    "        \n",
    "        return map_model[best_model]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ecdbc",
   "metadata": {},
   "source": [
    "# 6. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(symbol=None):\n",
    "    api = BinanceAPI(api_key, api_secret)\n",
    "    print(\"Available symbols:\")\n",
    "    for s in api.get_n_symbol(10):\n",
    "        print(s)\n",
    "    if not symbol:\n",
    "        # symbol = input(\"Symbol: \").strip().upper()\n",
    "        if not symbol:\n",
    "            symbol = \"ETHBTC\"\n",
    "    print(f\"\\nSelected symbol: {symbol}\")\n",
    "\n",
    "    ind_train_df, ind_test_df = create_train_test_ind_df(symbol)\n",
    "    if ind_train_df.shape[0] == 0 or ind_test_df.shape[0] == 0:\n",
    "        print(f\"{symbol} is invalid or has insufficient data. Try again\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nTrain set size: {ind_train_df.shape}, Test set size: {ind_test_df.shape}\")\n",
    "\n",
    "    best_model, best_features = get_best_model(ind_train_df)\n",
    "\n",
    "    x_test = ind_test_df[best_features]\n",
    "    y_test = ind_test_df['price_direction']\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    print(\"\\n=== Test Result ===\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
